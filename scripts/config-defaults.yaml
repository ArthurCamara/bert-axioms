seed:
  desc: Random seed
  value: 42
bert_class:
  desc: Class to be used on bert and on tokenizer
  value: "distilbert-base-uncased"
data_home:
  desc: Where to save data to
  value: "/ssd2/arthur/bert-axioms"
download_path:
  desc: Default msmarco download path
  value: "https://msmarco.blob.core.windows.net/msmarcoranking/"
logging_level:
  desc: Level for Python logger
  value: "INFO"
force_steps:
  desc: Steps to forecefully run
  value: []
train_queries:
  desc: Number of train queries expected
  value: 367013
full_dev_queries:
  desc: Number of dev queries in the original dataset
  value: 5193
corpus_size:
  desc: Number of documents in the corpus
  value: 3213835
number_of_cpus:
  desc: Number of CPUs to be used on parallel bits
  value: 48
max_document_len:
  desc: Maximum number of tokens to be considered in the document for the cut-down version of the dataset
  value: 500
indri_bin_path:
  desc: Path wihere Indri is installed
  value: "/ssd2/arthur/indri/bin"
split_percentage:
  desc: Percentage for test splitting
  value: .3
metric:
  desc: Main metric to be reported using trec_eval
  value: "ndcg"
indri_top_k:
  desc: Number of documents to be retrieved by Indri to re-rank
  value: 100
trec_eval_path:
  desc: Path for trec_eval
  value: "/ssd2/arthur/trec_eval/trec_eval"
test_set_size:
  desc: Size of the test dataset. 30% of the full_dev_queries, if split_percentage is .3.
  value: 1558 
negative_samples:
  desc: Number of negative samples for each positive sample
  value: 10
ignore_gpu_ids:
  desc: GPU IDs to ignore when running
  value: [0,1]
train_batch_size:
  desc: Size of training batch. Make sure it fits in memory!
  value: 128
n_epochs:
  desc: Number of epochs to train BERT
  value: 3
learning_rate:
  desc: BERT learning rate
  value: 0.00005