import os
import torch
from torch.utils.data import Dataset, DataLoader
from pytorch_transformers import BertTokenizer
from tqdm.auto import tqdm
import multiprocessing
multiprocessing.set_start_method('spawn', True)


class InputFeatures(object):
    """A single set of features of data."""

    def __init__(self, input_ids, input_mask, segment_ids, label_id):
        self.input_ids = torch.Tensor(input_ids, dtype=torch.long)
        self.input_mask = torch.Tensor(input_mask, dtype=torch.long)
        self.segment_ids = torch.Tensor(segment_ids, dtype=torch.long)
        self.label_id = torch.Tensor(label_id, dtype=torch.long)


class MsMarcoDataset(Dataset):
    """MsMarco preprocessing Dataset"""

    def __init__(
            self,
            tsv_file: str,
            data_dir: str,
            max_seq_len=512,
            size=None,
            transform=None,
            invert_label=True,
            XLNet=False):
        """
        Args:
            tsv_file (string): TSV file with triples, generated by text_to_tokens_slurm.py, formatted like
                "qid-did    bert-formatted-tokens   label"
            data_dir (string): Directory with the rest of the data, where we can write to
            size (int, optional): Number of lines to process. If None, will run wc -l first.
            transform (callable, optional): Transformations to be performed on the data
            invert_label: Must be True if you are planning to use BertForNextSentencePrediction
            XLNet: If you plan on using XLNet insted of BERT, the [CLS] token must be changed.
                Set this to True if you plan to. (NOT YET IMPLEMENTED)
        """
        self.tsv_path = tsv_file
        self.data_dir = data_dir
        self.transform = transform
        self.max_seq_len = max_seq_len
        if size is None:
            with open(tsv_file) as f:
                for i, _ in enumerate(f):
                    pass
            self.size = i + 1
        else:
            self.size = size

        self.offset_dict, self.index_dict = self.load_offset_dict()
        assert os.path.isdir(os.path.join(self.data_dir, "models"))
        self.tokenizer = BertTokenizer.from_pretrained(
            os.path.join(self.data_dir, "models"))
        if invert_label:
            self.label_map = {label: i for i, label in enumerate(["1", "0"])}
        else:
            self.label_map = {label: i for i, label in enumerate(["0", "1"])}

    def load_offset_dict(self):
        offset_dict = {}
        index_dict = {}
        with open(self.tsv_path, encoding="utf-8") as f:
            pbar = tqdm(total=self.size, desc="Computing offset dictionary")
            location = f.tell()
            line = f.readline()
            pbar.update()
            idx = 0
            while line:
                [did, _, _] = line.split("\t")
                offset_dict[did] = location
                index_dict[idx] = did
                location = f.tell()
                line = f.readline()
                pbar.update()
                idx += 1

        return offset_dict, index_dict

    def __getitem__(self, did):
        if isinstance(did, str):
            offset = self.offset_dict[did]
        elif isinstance(did, int):
            offset = self.offset_dict[self.index_dict[did]]
        else:
            raise NotImplementedError(
                "can only fetch integer or string indexes")
        with open(self.tsv_path) as f:
            f.seek(offset)
            line = f.readline()
        return self.text_to_features(line)

    def __len__(self):
        return self.size

    def _truncate_seq_pair(self, tokens_a, tokens_b, max_length):
        """Truncates a sequence pair in place to the maximum length."""
        while True:
            total_length = len(tokens_a) + len(tokens_b)
            if total_length <= max_length:
                break
            if len(tokens_a) > len(tokens_b):
                tokens_a.pop()
            else:
                tokens_b.pop()

    def text_to_features(self, sample):

        line = sample.strip().split("\t")
        tokens = eval(line[1])
        label = line[-1]

        sep_index = tokens.index("[SEP]")

        tokens_a = tokens[1:sep_index]
        tokens_b = tokens[sep_index+1:-1]

        segment_ids = [0] * (len(tokens_a) + 2)
        segment_ids += [1] * (len(tokens_b) + 1)

        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)
        input_mask = [1] * len(input_ids)
        padding = [0] * (self.max_seq_len - len(input_ids))
        input_ids += padding
        input_mask += padding
        segment_ids += padding

        assert len(input_ids) == self.max_seq_len, "input_id"
        assert len(input_mask) == self.max_seq_len, "input_mask"
        assert len(segment_ids) == self.max_seq_len, "segment_ids"

        label_id = self.label_map[label]
        return (
            torch.tensor(input_ids, dtype=torch.long),
            torch.tensor(input_mask, dtype=torch.long),
            torch.tensor(segment_ids, dtype=torch.long),
            torch.tensor([label_id], dtype=torch.long))


if __name__ == "__main__":
    dataset = MsMarcoDataset(
        "/ssd2/arthur/insy/msmarco/data/dev-triples.0", "/ssd2/arthur/TREC2019/data")
    data_loader = DataLoader(
        dataset, batch_size=32, shuffle=True, num_workers=4)