import os
import torch
from torch.utils.data import Dataset, DataLoader
from pytorch_transformers import BertTokenizer, DistilBertTokenizer
from tqdm.auto import tqdm
import logging
import pickle


class MsMarcoDataset(Dataset):
    """MsMarco preprocessing Dataset"""

    def __init__(
            self,
            tsv_file: str,
            data_dir: str,
            max_seq_len=512,
            labeled=True,
            size=None,
            transform=None,
            invert_label=True,
            xlnet=False,
            distil=False, 
            force=False):
        """
        Args:
            tsv_file (string): TSV file with triples, generated by text_to_tokens_slurm.py, formatted like
                "qid-did    bert-formatted-tokens   label"
            data_dir (string): Directory with the rest of the data, where we can write to
            size (int, optional): Number of lines to process. If None, will run wc -l first.
            transform (callable, optional): Transformations to be performed on the data
            invert_label: Must be True if you are planning to use BertForNextSentencePrediction
            distil: Use distlBERT?
            force: Force to re-compute offset dicts and size
        """
        logging.info("Loading dataset from %s", tsv_file)
        self.tsv_path = tsv_file
        self.data_dir = data_dir
        self.transform = transform
        self.max_seq_len = max_seq_len
        self.xlnet = xlnet
        self.distil = distil
        self.labeled = labeled
        if tsv_file.endswith("train-triples.top100"):
            size = 361276621
        elif tsv_file.endswith("train-triples.10neg"):
            size = 2935878
        # if tsv_file.endswith("cut-train.10neg"):
            # size = 4037132
        # elif tsv_file.endswith("cut-dev.10neg"):
            # size = 39974
        if size is None or force:
            with open(tsv_file, encoding="utf-8", errors="ignore") as f:
                for i, _ in tqdm(enumerate(f), desc="Counting lines on file..."):
                    pass
            self.size = i + 1
        else:
            self.size = size

        self.offset_dict, self.index_dict = self.load_offset_dict(force)
        assert os.path.isdir(os.path.join(self.data_dir, "models"))
        if distil:
            self.tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
        else:
            self.tokenizer = BertTokenizer.from_pretrained(
                os.path.join(self.data_dir, "models"))
        if invert_label:
            self.label_map = {label: i for i, label in enumerate(["1", "0"])}
        else:
            self.label_map = {label: i for i, label in enumerate(["0", "1"])}

    def load_offset_dict(self, force=False):
        offset_dict = {}
        index_dict = {}
        cache_file = self.tsv_path + ".offset"
        index_file = self.tsv_path + ".index"
        if not force and (os.path.isfile(cache_file) and os.path.isfile(index_file)):
            offset_dict = pickle.load(open(cache_file, 'rb'))
            index_dict = pickle.load(open(index_file, 'rb'))
            return offset_dict, index_dict

        with open(self.tsv_path, encoding="utf-8", errors="ignore") as f:
            pbar = tqdm(total=self.size, desc="Computing offset dictionary")
            location = f.tell()
            line = f.readline().encode("utf-8")
            pbar.update()
            idx = 0
            while line:
                if len(line) < 2:
                    line = f.readline().encode("utf-8")
                    pbar.update()
                    location = f.tell()
                    self.size -= 1
                    continue
                if self.labeled:
                    [did, _, _] = line.decode().split("\t")
                else:
                    [did, _] = line.decode().split("\t")
                offset_dict[did] = location
                index_dict[idx] = did
                location = f.tell()
                line = f.readline().encode("utf-8")
                pbar.update()
                idx += 1
        with open(cache_file, 'wb') as f:
            pickle.dump(offset_dict, f)
        with open(index_file, 'wb') as f:
            pickle.dump(index_dict, f)

        return offset_dict, index_dict

    def __getitem__(self, did):
        if isinstance(did, str):
            offset = self.offset_dict[did]
        elif isinstance(did, int):
            offset = self.offset_dict[self.index_dict[did]]
        elif isinstance(did, slice):
            return [self[_id] for _id in range(did.start, did.stop)]
        with open(self.tsv_path, encoding="utf-8", errors="ignore") as f:
            f.seek(offset)
            line = f.readline().encode("utf-8")
        return self.text_to_features(line)

    def __len__(self):
        return self.size

    def text_to_features(self, sample):

        line = sample.strip().decode().split("\t")
        tokens = eval(line[1])
        if self.labeled:
            label = line[-1]
        else:
            label = None

        sep_index = tokens.index("[SEP]")

        tokens_a = tokens[1:sep_index]
        tokens_b = tokens[sep_index + 1:-1]

        segment_ids = [0] * (len(tokens_a) + 2)
        segment_ids += [1] * (len(tokens_b) + 1)

        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)
        input_mask = [1] * len(input_ids)
        padding = [0] * (self.max_seq_len - len(input_ids))
        input_ids += padding
        input_mask += padding
        segment_ids += padding

        assert len(input_ids) == self.max_seq_len, "input_id"
        assert len(input_mask) == self.max_seq_len, "input_mask"
        assert len(segment_ids) == self.max_seq_len, "segment_ids"

        if self.labeled:
            label_id = self.label_map[label]
            return (
                torch.tensor(input_ids, dtype=torch.long),
                torch.tensor(input_mask, dtype=torch.long),
                torch.tensor(segment_ids, dtype=torch.long),
                torch.tensor([label_id], dtype=torch.long))
        else:
            return (
                torch.tensor(input_ids, dtype=torch.long),
                torch.tensor(input_mask, dtype=torch.long),
                torch.tensor(segment_ids, dtype=torch.long))


if __name__ == "__main__":
    dataset = MsMarcoDataset(
        "/ssd2/arthur/TREC2019/data/triples-tokenized/cut-test.top100", "/ssd2/arthur/TREC2019/data", force=True, labeled=True)
    dataloader = DataLoader(dataset, batch_size=1024, shuffle=False)
    for index, batch in tqdm(enumerate(dataloader), desc="{} Dataset".format(dataset), total=len(dataloader)):
        pass
