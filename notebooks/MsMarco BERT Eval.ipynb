{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T10:54:26.972058Z",
     "start_time": "2019-08-07T10:54:26.849500Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm import tnrange as trange\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "\n",
    "if sys.version_info[0] == 2:\n",
    "    import cPickle as pickle\n",
    "else:\n",
    "    import pickle\n",
    "\n",
    "from pytorch_transformers import BertForNextSentencePrediction, BertTokenizer\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "sys.path.append(\"../scripts/\")\n",
    "from run_classifier_dataset_utils import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T09:10:13.325697Z",
     "start_time": "2019-08-07T09:10:13.253426Z"
    }
   },
   "outputs": [],
   "source": [
    "task_name = \"msmarco\"\n",
    "do_train = False\n",
    "do_eval = True\n",
    "do_lower_case = True\n",
    "data_dir = \"/ssd2/arthur/TREC2019/data/\"\n",
    "bert_model = \"bert-base-uncased\"\n",
    "max_seq_length = 512 \n",
    "train_batch_size = 32\n",
    "learning_rate = 2e-5 \n",
    "num_train_epochs = 3.0 \n",
    "output_dir = os.path.join(data_dir, \"models\")\n",
    "overwrite_output_dir = False\n",
    "eval_batch_size = 128\n",
    "\n",
    "local_rank = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T09:10:14.162971Z",
     "start_time": "2019-08-07T09:10:14.107698Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T09:10:19.657648Z",
     "start_time": "2019-08-07T09:10:14.825420Z"
    }
   },
   "outputs": [],
   "source": [
    "model = BertForNextSentencePrediction.from_pretrained(output_dir)\n",
    "model = torch.nn.DataParallel(model)\n",
    "tokenizer = BertTokenizer.from_pretrained(output_dir, do_lower_case=do_lower_case)\n",
    "model.to(device)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T10:53:32.144656Z",
     "start_time": "2019-08-07T10:53:14.708423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5191671 /ssd2/arthur/TREC2019/data/bm25_bert_docs.tsv\r\n"
     ]
    }
   ],
   "source": [
    "!wc -l /ssd2/arthur/TREC2019/data/bm25_bert_docs.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T11:13:36.924680Z",
     "start_time": "2019-08-07T11:13:34.038731Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/ssd2/arthur/TREC2019/data/dev_bert-base-uncased_512_msmarco\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5589a06e34e0421fa54c370605957667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Reading input tsv', max=2122814, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-7a7ac560a0de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_reload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# assert len(eval_examples) == 5191671\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ssd2/arthur/TREC2019/scripts/run_classifier_dataset_utils.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(task_name, model_name, max_seq_length, data_dir, tokenizer, batch_size, eval, sample, return_examples, force_reload)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtask_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dev_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_train_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ssd2/arthur/TREC2019/scripts/run_classifier_dataset_utils.py\u001b[0m in \u001b[0;36mget_dev_examples\u001b[0;34m(self, data_dir)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_dev_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         return self._create_examples(\n\u001b[0;32m---> 81\u001b[0;31m             self._read_tsv(os.path.join(data_dir, \"bm25_bert_docs.tsv\")), \"dev\")\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/ssd2/arthur/TREC2019/scripts/run_classifier_dataset_utils.py\u001b[0m in \u001b[0;36m_read_tsv\u001b[0;34m(cls, input_file, quotechar, sample)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquotechar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquotechar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Reading input tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2122814\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bert/lib/python3.7/site-packages/tqdm/_tqdm_notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bert/lib/python3.7/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                 \"\"\"), fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1004\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1005\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1006\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eval_dataloader, eval_examples = load_dataset(task_name, bert_model, max_seq_length, data_dir, tokenizer, eval_batch_size, eval=True, return_examples=True, force_reload=False)\n",
    "\n",
    "# assert len(eval_examples) == 5191671"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T10:56:29.396571Z",
     "start_time": "2019-08-07T10:56:29.296671Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9360, device='cuda:0')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T10:58:55.180449Z",
     "start_time": "2019-08-07T10:58:55.082011Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([106, 512])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-07T10:40:46.521Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17fb76baabfe4624ac97e76565a692f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=454, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "eval_loss = 0\n",
    "nb_eval_steps = 0\n",
    "preds = []\n",
    "out_label_ids = None\n",
    "scores = []\n",
    "classes = []\n",
    "evaluated_samples = 0 \n",
    "\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    label_ids = label_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, token_type_ids=segment_ids, next_sentence_label=label_ids)\n",
    "        predictions = outputs[1]\n",
    "        eval_loss += sum(outputs[0])\n",
    "        \n",
    "        scores += list(predictions[:, 0].cpu().detach().numpy())\n",
    "        \n",
    "        classes += list(torch.argmax(predictions, dim=1).cpu().numpy())\n",
    "\n",
    "        nb_eval_steps+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-08-07T10:41:02.504Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "from collections import defaultdict\n",
    "#load bm25 scores.\n",
    "bm25_scores = {}\n",
    "bm25_run_file = \"/ssd2/arthur/insy/msmarco/data/results/dev/bm25_finetuned.run\"\n",
    "guids = []\n",
    "last_topic = None\n",
    "normalized_scores = []\n",
    "ordered_topics = []\n",
    "scores_per_topic = defaultdict(lambda:[])\n",
    "\n",
    "\n",
    "with open(bm25_run_file, 'r') as inf:\n",
    "    for counter, line in tqdm(enumerate(inf), desc=\"reading run file\"):\n",
    "        [topic_id, _, doc_id, _, score, _] = line.split()\n",
    "        if topic_id not in ordered_topics:\n",
    "            ordered_topics.append(topic_id)\n",
    "        scores_per_topic[topic_id].append((doc_id, score))\n",
    "#normalize\n",
    "for _id in tqdm(scores_per_topic, desc=\"normalizing\"):\n",
    "    _scores = np.asarray([float(x[1]) for x in scores_per_topic[_id]])\n",
    "    normalized_scores = (_scores - np.min(_scores))/np.ptp(_scores)\n",
    "    for (did, _), score in zip(scores_per_topic[_id], normalized_scores):\n",
    "        guid = \"{}-{}\".format(_id, did)\n",
    "        bm25_scores[guid] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T10:52:30.204381Z",
     "start_time": "2019-08-07T10:52:30.125927Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58090"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T10:38:02.032645Z",
     "start_time": "2019-08-07T10:38:01.639105Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## from IPython.core.debugger import set_trace\n",
    "\n",
    "import subprocess\n",
    "\n",
    "trec_path = \"/ssd2/arthur/trec_eval/trec_eval\"\n",
    "qrel_path = \"/ssd2/arthur/TREC2019/data/msmarco-docdev-qrels.tsv\"\n",
    "cmd = \"{} -q -c {} {}\"\n",
    "\n",
    "\n",
    "\n",
    "best_map = 0.0\n",
    "\n",
    "runs_format = \"{} Q0 {} {} {} BERT_BM25\\n\" #topic_id, doc_id, ranking, score\n",
    "\n",
    "n_alphas = 50\n",
    "for a in range(0, n_alphas):\n",
    "    alpha = a/n_alphas\n",
    "    beta = 1-alpha\n",
    "\n",
    "    run_file = os.path.join(\"/ssd2/arthur/TREC2019/data/runs/bert_eval/bert-sequence-{}.res\".format(alpha))\n",
    "\n",
    "    topic_results = []\n",
    "    last_topic = eval_examples[0].guid.split(\"-\")[1]\n",
    "    with open(run_file, 'w') as outf, open(bm25_run_file) as inf:\n",
    "        for counter, (example, score) in enumerate(zip(eval_examples, scores)):\n",
    "#             print(example.guid, score)\n",
    "            [_, topic_id, doc_id] = example.guid.split(\"-\")\n",
    "            if topic_id != last_topic:\n",
    "                last_topic = topic_id\n",
    "                print(topic_id)\n",
    "                break\n",
    "#                 topic_results.sort(key = lambda x:x['score'], reverse=True)\n",
    "#                 for rank, topic in enumerate(topic_results):\n",
    "#                     outf.write(runs_format.format(topic['topic_id'], topic['doc_id'], rank, topic['score']))\n",
    "#                 topic_results = []\n",
    "#             topic_results.append({'topic_id': topic_id, 'doc_id': doc_id, 'score': alpha*score+beta*bm25_scores[f\"{topic_id}-{doc_id}\"]})\n",
    "#             last_topic = topic_id\n",
    "#         for rank, topic in enumerate(topic_results):\n",
    "#             outf.write(runs_format.format(topic['topic_id'], topic['doc_id'], rank, topic['score']))\n",
    "#     result = subprocess.check_output(cmd.format(trec_path, qrel_path, run_file).split()).decode('utf-8')\n",
    "#     _map = float(result.split(\"\\n\")[-26].split(\"\\t\")[-1])\n",
    "#     print(\"alpha: {}\\t map: {}\".format(alpha, _map))\n",
    "#     if _map > best_map:\n",
    "#         best_map = _map\n",
    "#         print(\"best map found for alpha {}, map={}\".format(alpha, _map))\n",
    "#         best_file = run_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-07T09:41:54.481977Z",
     "start_time": "2019-08-07T09:41:51.580Z"
    }
   },
   "outputs": [],
   "source": [
    "# eval script:\n",
    "cmd = \"/ssd2/arthur/terrier-core/bin/terrier batchevaluate -f -q {}\".format(os.path.join(data_dir, \"msmarco-docdev-qrels.tsv\"))\n",
    "output = subprocess.run(cmd.split(), capture_output=True)\n",
    "lines = output.stdout.decode(\"utf-8\").split(\"\\n\")[3:-1]\n",
    "max_score = 0.0\n",
    "for i, j in list(zip(lines[:-1], lines[1:]))[::2]:\n",
    "    alpha = i.split(\"-\")[-1].split(\".res\")[0]\n",
    "    score = float(j.split(\":\")[-1])\n",
    "    print(alpha, score)\n",
    "    if score > max_score:\n",
    "        max_score = score\n",
    "        best_alpha = alpha\n",
    "print(best_alpha, max_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T18:25:06.390450Z",
     "start_time": "2019-07-22T18:25:06.292889Z"
    }
   },
   "outputs": [],
   "source": [
    "26.01846062624611"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T18:26:02.010480Z",
     "start_time": "2019-07-22T18:26:01.915996Z"
    }
   },
   "outputs": [],
   "source": [
    "example.guid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T18:17:28.969205Z",
     "start_time": "2019-07-22T18:17:28.905378Z"
    }
   },
   "outputs": [],
   "source": [
    "[x for x in topic_results if x['doc_id']==\"D3240836\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
