{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T07:01:58.335698Z",
     "start_time": "2019-08-06T07:01:58.289846Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from tqdm import tnrange as trange\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "\n",
    "\n",
    "if sys.version_info[0] == 2:\n",
    "    import cPickle as pickle\n",
    "else:\n",
    "    import pickle\n",
    "\n",
    "from pytorch_transformers import BertForNextSentencePrediction, BertTokenizer\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "sys.path.append(\"../scripts/run_classifier_dataset_utils.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T06:53:27.159656Z",
     "start_time": "2019-08-06T06:53:27.151622Z"
    }
   },
   "outputs": [],
   "source": [
    "task_name = \"msmarco\"\n",
    "do_train = False\n",
    "do_eval = True\n",
    "do_lower_case = True\n",
    "data_dir = \"/ssd2/arthur/TREC2019/data/\"\n",
    "bert_model = \"bert-base-uncased\"\n",
    "max_seq_length = 512 \n",
    "train_batch_size = 32\n",
    "learning_rate = 2e-5 \n",
    "num_train_epochs = 3.0 \n",
    "output_dir = os.path.join(data_dir, \"models\")\n",
    "overwrite_output_dir = False\n",
    "eval_batch_size = 128\n",
    "\n",
    "local_rank = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T06:53:27.962086Z",
     "start_time": "2019-08-06T06:53:27.957284Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T06:53:34.254967Z",
     "start_time": "2019-08-06T06:53:28.248909Z"
    }
   },
   "outputs": [],
   "source": [
    "model = BertForNextSentencePrediction.from_pretrained(output_dir)\n",
    "model = torch.nn.DataParallel(model)\n",
    "tokenizer = BertTokenizer.from_pretrained(output_dir, do_lower_case=do_lower_case)\n",
    "model.to(device)\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T07:11:50.714077Z",
     "start_time": "2019-08-06T07:11:50.629199Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<run_classifier_dataset_utils.InputFeatures at 0x7f66a2071390>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T07:30:14.941435Z",
     "start_time": "2019-08-06T07:30:01.180531Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading input tsv:   3%|▎         | 58091/2122814 [00:08<04:47, 7187.93it/s]\n",
      "creating examples...: 58091it [00:00, 328399.81it/s]\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from run_classifier_dataset_utils import load_dataset\n",
    "eval_dataloader, eval_examples = load_dataset(task_name, bert_model, max_seq_length, data_dir, tokenizer, eval_batch_size, eval=True, return_examples=True)\n",
    "assert eval_examples[0].guid == \"dev-174249-D3126537\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-06T07:30:24.447654Z",
     "start_time": "2019-08-06T07:30:24.327350Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dev-174249-D3126537'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_examples[0].guid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T21:19:27.802498Z",
     "start_time": "2019-07-22T21:15:11.701856Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bff5d7316254b8dbee81129f8169645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=391, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "eval_loss = 0\n",
    "nb_eval_steps = 0\n",
    "preds = []\n",
    "out_label_ids = None\n",
    "scores = []\n",
    "classes = []\n",
    "\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "for input_ids, input_mask, segment_ids, label_ids in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "    input_ids = input_ids.to(device)\n",
    "    input_mask = input_mask.to(device)\n",
    "    segment_ids = segment_ids.to(device)\n",
    "    label_ids = label_ids.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, token_type_ids=segment_ids, next_sentence_label=label_ids)\n",
    "        predictions = outputs[1]\n",
    "        eval_loss += outputs[0]\n",
    "        \n",
    "        scores += list(predictions[:, 1].cpu().detach().numpy())\n",
    "        \n",
    "        classes += list(torch.argmax(predictions, dim=1).cpu().numpy())\n",
    "\n",
    "        nb_eval_steps+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T21:04:00.553775Z",
     "start_time": "2019-07-22T21:04:00.359686Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "from collections import defaultdict\n",
    "#load bm25 scores. Fnal score is a combination THIS IS BUGGY\n",
    "bm25_scores = {}\n",
    "bm25_run_file = \"/ssd2/arthur/terrier-core/var/results/run.msmarco_docs.bm25.res\"\n",
    "guids = []\n",
    "last_topic = None\n",
    "normalized_scores = []\n",
    "ordered_topics = []\n",
    "scores_per_topic = defaultdict(lambda:[])\n",
    "\n",
    "\n",
    "with open(bm25_run_file, 'r') as inf:\n",
    "    for counter, line in enumerate(inf):\n",
    "        [topic_id, _, doc_id, _, score, _] = line.split()\n",
    "        if topic_id not in ordered_topics:\n",
    "            ordered_topics.append(topic_id)\n",
    "        scores_per_topic[topic_id].append((doc_id, score))\n",
    "#normalize\n",
    "for _id in scores_per_topic:\n",
    "    scores = np.asarray([float(x[1]) for x in scores_per_topic[_id]])\n",
    "    normalized_scores = (scores - np.min(scores))/np.ptp(scores)\n",
    "    for (did, _), score in zip(scores_per_topic[_id], normalized_scores):\n",
    "        guid = \"{}-{}\".format(_id, did)\n",
    "        bm25_scores[guid] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T21:20:26.128873Z",
     "start_time": "2019-07-22T21:20:01.289722Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.2735\n",
      "0.02 0.2926\n",
      "0.04 0.3067\n",
      "0.06 0.3073\n",
      "0.08 0.3128\n",
      "0.1 0.3327\n",
      "0.12 0.3426\n",
      "0.14 0.3357\n",
      "0.16 0.3561\n",
      "0.18 0.3592\n",
      "0.2 0.3581\n",
      "0.22 0.3592\n",
      "0.24 0.354\n",
      "0.26 0.3511\n",
      "0.28 0.3493\n",
      "0.3 0.3496\n",
      "0.32 0.3475\n",
      "0.34 0.3484\n",
      "0.36 0.3438\n",
      "0.38 0.3371\n",
      "0.4 0.3353\n",
      "0.42 0.3335\n",
      "0.44 0.3258\n",
      "0.46 0.3023\n",
      "0.48 0.2934\n",
      "0.5 0.2816\n",
      "0.52 0.2664\n",
      "0.54 0.2586\n",
      "0.56 0.2561\n",
      "0.58 0.2549\n",
      "0.6 0.2402\n",
      "0.62 0.2363\n",
      "0.64 0.2144\n",
      "0.66 0.2123\n",
      "0.68 0.207\n",
      "0.7 0.204\n",
      "0.72 0.2022\n",
      "0.74 0.2001\n",
      "0.76 0.1981\n",
      "0.78 0.1948\n",
      "0.8 0.184\n",
      "0.82 0.1779\n",
      "0.84 0.1654\n",
      "0.86 0.1578\n",
      "0.88 0.1574\n",
      "0.9 0.1535\n",
      "0.92 0.1509\n",
      "0.94 0.1461\n",
      "0.96 0.145\n",
      "0.98 0.1442\n",
      "0.18 0.3592\n"
     ]
    }
   ],
   "source": [
    "## from IPython.core.debugger import set_trace\n",
    "\n",
    "import subprocess\n",
    "\n",
    "runs_format = \"{} Q0 {} {} {} BERT_BM25\\n\" #topic_id, doc_id, ranking, score\n",
    "\n",
    "n_alphas = 50\n",
    "for a in range(0, n_alphas):\n",
    "    alpha = a/n_alphas\n",
    "    beta = 1-alpha\n",
    "\n",
    "    run_file = os.path.join(\"/ssd2/arthur/terrier-core/var/results/bert-{}.res\".format(alpha))\n",
    "\n",
    "    topic_results = []\n",
    "    last_topic = eval_examples[0].guid.split(\"-\")[1]\n",
    "    with open(run_file, 'w') as outf, open(bm25_run_file) as inf:\n",
    "        for counter, (example, score) in enumerate(zip(eval_examples, scores)):\n",
    "            [_, topic_id, doc_id] = example.guid.split(\"-\")\n",
    "            if topic_id != last_topic:\n",
    "                last_topic = topic_id\n",
    "                topic_results.sort(key = lambda x:x['score'], reverse=True)\n",
    "                for rank, topic in enumerate(topic_results):\n",
    "                    outf.write(runs_format.format(topic['topic_id'], topic['doc_id'], rank, topic['score']))\n",
    "                topic_results = []\n",
    "            topic_results.append({'topic_id': topic_id, 'doc_id': doc_id, 'score': alpha*score+beta*bm25_scores[f\"{topic_id}-{doc_id}\"]})\n",
    "            last_topic = topic_id\n",
    "        for rank, topic in enumerate(topic_results):\n",
    "            outf.write(runs_format.format(topic['topic_id'], topic['doc_id'], rank, topic['score']))\n",
    "\n",
    "# eval script:\n",
    "cmd = \"/ssd2/arthur/terrier-core/bin/terrier batchevaluate -f -q {}\".format(os.path.join(data_dir, \"msmarco-docdev-qrels.tsv\"))\n",
    "output = subprocess.run(cmd.split(), capture_output=True)\n",
    "lines = output.stdout.decode(\"utf-8\").split(\"\\n\")[3:-1]\n",
    "max_score = 0.0\n",
    "for i, j in list(zip(lines[:-1], lines[1:]))[::2]:\n",
    "    alpha = i.split(\"-\")[-1].split(\".res\")[0]\n",
    "    score = float(j.split(\":\")[-1])\n",
    "    print(alpha, score)\n",
    "    if score > max_score:\n",
    "        max_score = score\n",
    "        best_alpha = alpha\n",
    "print(best_alpha, max_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T19:32:29.420165Z",
     "start_time": "2019-07-22T19:32:29.300627Z"
    }
   },
   "outputs": [],
   "source": [
    "lines[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T18:25:06.390450Z",
     "start_time": "2019-07-22T18:25:06.292889Z"
    }
   },
   "outputs": [],
   "source": [
    "26.01846062624611"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T18:26:02.010480Z",
     "start_time": "2019-07-22T18:26:01.915996Z"
    }
   },
   "outputs": [],
   "source": [
    "example.guid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-22T18:17:28.969205Z",
     "start_time": "2019-07-22T18:17:28.905378Z"
    }
   },
   "outputs": [],
   "source": [
    "[x for x in topic_results if x['doc_id']==\"D3240836\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
