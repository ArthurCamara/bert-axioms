{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T23:53:34.004218Z",
     "start_time": "2019-10-10T23:53:32.218834Z"
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from pytorch_transformers import BertTokenizer\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm.auto import tqdm\n",
    "from itertools import product\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.similarities import SoftCosineSimilarity\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "import random\n",
    "import re\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--data_home\", type=str, default=\"/ssd2/arthur/TREC2019/data\")\n",
    "parser.add_argument(\"--docs_file\", type=str, default=\"docs/tokenized-msmarco-docs.tsv\")\n",
    "parser.add_argument(\"--queries_file\", type=str, default=\"queries/test_queries.tsv\")\n",
    "parser.add_argument(\"--axioms\", type=str, default=\"TFC1,TFC2,MTDC,LNC1,LNC2,LB1,LB2,STMC1,STMC2,STMC3,TP\")\n",
    "parser.add_argument(\"--top100\", type=str, default=\"runs/indri_test_10_10.run\")\n",
    "parser.add_argument(\"--cpus\", type=int, default=1)\n",
    "parser.add_argument(\"--total_docs\", type=int, default=3213835)\n",
    "parser.add_argument(\"--delta\", type=int, default=100)\n",
    "parser.add_argument(\"--idf_file\", type=str, default=\"docs/IDFS/IDFS-FULL\")\n",
    "parser.add_argument(\"--embeddings_path\", type=str, default=\"GloVe/w2v.txt\")\n",
    "parser.add_argument(\"--stmc_sim\", type=float, default=0.2)\n",
    "parser.add_argument(\"--delta_lb\", type=float, default=0.2)\n",
    "parser.add_argument(\"--min_df_lb\", type=int, default=100000)\n",
    "\n",
    "argv = [\"--cpus\", \"1\",\n",
    "        \"--axioms\", \"TFC1\"]\n",
    "args = parser.parse_args(argv)\n",
    "sys.path.insert(0, \"/ssd2/arthur/TREC2019/scripts/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T23:54:21.836642Z",
     "start_time": "2019-10-10T23:53:35.104830Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d174e36a254d538dc6044cb4e5c9fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=155800), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def getcontent(doc_id, docs_file, offset_dict):\n",
    "    offset = offset_dict[doc_id]\n",
    "    with open(docs_file) as f:\n",
    "        f.seek(offset)\n",
    "        doc = f.readline()\n",
    "    return doc\n",
    "\n",
    "docs_path = os.path.join(args.data_home, args.docs_file)\n",
    "offset_dict = pickle.load(open(docs_path+\".offset\", 'rb'))\n",
    "top_100_path = os.path.join(args.data_home, args.top100)\n",
    "assert os.path.isfile(top_100_path)\n",
    "assert os.path.isfile(docs_path)\n",
    "#set of valid documents\n",
    "tuples = defaultdict(lambda: set())\n",
    "all_docs = {}\n",
    "docs_lens = {}\n",
    "scores = {}\n",
    "for i in tqdm(open(top_100_path), total=155800):\n",
    "    topic_id, _ , doc_id, _, score, _ = i.split()\n",
    "    tuples[topic_id].add(doc_id)\n",
    "    scores[f\"{topic_id}-{doc_id}\"] = score\n",
    "    if doc_id in all_docs:\n",
    "        continue  \n",
    "    all_docs[doc_id] = getcontent(doc_id, docs_path, offset_dict).split()\n",
    "    docs_lens[doc_id] = len(all_docs[doc_id])\n",
    "args.scores = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T23:54:21.846897Z",
     "start_time": "2019-10-10T23:54:21.843724Z"
    }
   },
   "outputs": [],
   "source": [
    "#create a triples file with the instances needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T23:54:32.056596Z",
     "start_time": "2019-10-10T23:54:32.047640Z"
    }
   },
   "outputs": [],
   "source": [
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length=509):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "def text_to_tokens(query, document):\n",
    "    tokens_a = query\n",
    "    tokens_b = document\n",
    "    _truncate_seq_pair(tokens_a, tokens_b)\n",
    "    tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T23:55:12.532704Z",
     "start_time": "2019-10-10T23:55:11.479262Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "482af49ba3e44332aaa6d9031be32c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1558), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "queries_file = os.path.join(args.data_home, args.queries_file)\n",
    "tokenized_queries = {}\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "for line in tqdm(open(queries_file), total=1558):\n",
    "    q_id, query_text = line.strip().split(\"\\t\")\n",
    "    tokenized_query = tokenizer.tokenize(query_text)\n",
    "    tokenized_queries[q_id] = tokenized_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T23:55:20.170269Z",
     "start_time": "2019-10-10T23:55:13.904532Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a130c5c6991418682a52d72bf32da9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=155800), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "multiply_factor = dict()\n",
    "doc_lens = []\n",
    "processed_docs = set()\n",
    "processed_docs_per_topic = defaultdict(lambda: set())\n",
    "instances = []\n",
    "with open(\"/ssd2/arthur/TREC2019/data/triples-tokenized/LNC2_test-triples.top100\", 'w') as outf:\n",
    "    for line in tqdm(open(top_100_path), total = 155800):\n",
    "        topic_id, _ , D, _, score, _ = line.split()\n",
    "        doc = all_docs[D]\n",
    "        doc_text = \" \".join(doc)\n",
    "        if len(doc)<256:\n",
    "            processed_docs_per_topic[topic_id].add(D)\n",
    "            processed_docs.add(D)\n",
    "            multiply_factor[D] = 1\n",
    "            new_doc = doc.copy()\n",
    "            missing = 512 - len(new_doc)\n",
    "            while missing > len(doc):\n",
    "                multiply_factor[D] += 1\n",
    "                new_doc += doc\n",
    "                missing = 512 - len(new_doc)\n",
    "            doc = text_to_tokens(tokenized_queries[topic_id], doc)\n",
    "            outf.write(\"{}-{}\\t{}\\n\".format(topic_id, D, doc))\n",
    "            new_doc = text_to_tokens(tokenized_queries[topic_id], new_doc)\n",
    "            outf.write(\"{}-{}-LNC2\\t{}\\n\".format(topic_id, D, new_doc))\n",
    "            instances.append((topic_id, D, D+\"-LNC2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T23:55:49.337864Z",
     "start_time": "2019-10-10T23:55:49.322165Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.647257260193605, 1.5655546098891808, 2.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(list(multiply_factor.values())), np.std(list(multiply_factor.values())), np.median(list(multiply_factor.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T23:56:12.559839Z",
     "start_time": "2019-10-10T23:56:07.882433Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9e295f9f7b7478dbd07d971fee53b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Counting lines on file...', max=1, style=Pr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c587a0b66b5435eafbeb49c6e679e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Computing offset dictionary', max=14904, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2,3,4,5,6,7\"  # specify which GPU(s) to be used\n",
    "from pytorch_transformers import DistilBertForSequenceClassification\n",
    "import sys\n",
    "import torch\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"/ssd2/arthur/TREC2019/data/models/distilbert-cut/\")\n",
    "model = torch.nn.DataParallel(model)\n",
    "sys.path.insert(0, \"/ssd2/arthur/TREC2019/scripts/\")\n",
    "batch_size = 1024\n",
    "from msmarco_dataset import MsMarcoDataset\n",
    "from torch.utils.data import DataLoader\n",
    "dataset = MsMarcoDataset(\"/ssd2/arthur/TREC2019/data/triples-tokenized/LNC2_test-triples.top100\", args.data_home, distil=True, invert_label=True, labeled=False, force=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T23:57:48.436069Z",
     "start_time": "2019-10-10T23:56:22.813197Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adb95b8f32be43fc8ca2e5d5b67197e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='<msmarco_dataset.MsMarcoDataset object at 0x7fe1a5ca20f0> Dat…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "n_gpu = torch.cuda.device_count()\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available() and n_gpu > 0) else \"cpu\")\n",
    "model = model.to(device)\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "preds = {}\n",
    "eval_loss = 0.0\n",
    "nb_eval_steps = 0\n",
    "preds[dataset] = None\n",
    "out_label_ids = 0\n",
    "_preds = None\n",
    "for index, batch in tqdm(enumerate(dataloader), desc=\"{} Dataset\".format(dataset), total = len(dataloader)):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        inputs = {'input_ids': batch[0].to(device),\n",
    "                  'attention_mask': batch[1].to(device)}\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs[0]\n",
    "        if _preds is None:\n",
    "            _preds = logits.detach().cpu().numpy()\n",
    "        else:\n",
    "            batch_predictions = logits.detach().cpu().numpy()\n",
    "            _preds = np.append(_preds, batch_predictions, axis=0)\n",
    "torch.save(_preds, os.path.join(args.data_home, 'predictions', 'LNC2.distilBERT.tensor'))\n",
    "assert len(_preds) == len(instances)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T16:47:50.746788Z",
     "start_time": "2019-10-03T16:47:50.402748Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44ba04d2ddcb4d0ca97b0e47ccaa748a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='reading run file', max=13882, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95a66067c25a4663a75b59bb10a568cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='normalizing', max=1343, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "ql_scores = defaultdict(lambda:[])\n",
    "ordered_topics = []\n",
    "scores_per_topic = defaultdict(lambda:[])\n",
    "QL_run_file = \"/ssd2/arthur/TREC2019/data/runs/LNC.run\"\n",
    "last_topic = None\n",
    "normalized_scores = []\n",
    "with open(QL_run_file, 'r') as inf:\n",
    "    for counter, line in tqdm(enumerate(inf), desc=\"reading run file\", total=len(dataset)):\n",
    "        [topic_id, _, doc_id, _, score, _] = line.split()\n",
    "        if topic_id not in ordered_topics:\n",
    "            ordered_topics.append(topic_id)\n",
    "        scores_per_topic[topic_id].append((doc_id, score))\n",
    "assert sum([len(scores_per_topic[x]) for x in scores_per_topic]) == len(_preds)\n",
    "#normalize\n",
    "for _id in tqdm(scores_per_topic, desc=\"normalizing\"):\n",
    "    _scores = np.asarray([float(x[1]) for x in scores_per_topic[_id]])\n",
    "    normalized_scores = (_scores - np.min(_scores))/np.ptp(_scores)\n",
    "    for (did, _), score in zip(scores_per_topic[_id], normalized_scores):\n",
    "        guid = \"{}-{}\".format(_id, did)\n",
    "        ql_scores[guid] = score\n",
    "assert len(ql_scores) == len(_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T23:59:34.854470Z",
     "start_time": "2019-10-10T23:59:34.821584Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = softmax(torch.as_tensor(_preds))[:,0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T00:20:47.588459Z",
     "start_time": "2019-10-11T00:20:47.581931Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7452"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T00:18:02.531892Z",
     "start_time": "2019-10-11T00:18:02.528753Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1558"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qrels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T00:20:10.267890Z",
     "start_time": "2019-10-11T00:20:10.249012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n"
     ]
    }
   ],
   "source": [
    "qrels_path = os.path.join(args.data_home, 'qrels', \"test_qrels\")\n",
    "qrels = {}\n",
    "for line in open(qrels_path):\n",
    "    topic_id, _, doc_id, rel = line.split(\"\\t\")\n",
    "    if topic_id in qrels:\n",
    "        qrels[topic_id].append(doc_id)\n",
    "    else:\n",
    "        qrels[topic_id] = [doc_id]\n",
    "\n",
    "instances_with_relevant = 0\n",
    "for i in instances:\n",
    "    if i[1] in qrels[i[0]]:\n",
    "        instances_with_relevant += 1\n",
    "print(instances_with_relevant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-10T23:59:39.083993Z",
     "start_time": "2019-10-10T23:59:39.076184Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.055152979066022546"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fulfills = 0\n",
    "_list = list(zip(preds[:-1], preds[1:]))\n",
    "for v, w in _list[::2]:\n",
    "    if v<=w:\n",
    "        fulfills+=1\n",
    "fulfills/len(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T16:48:40.582577Z",
     "start_time": "2019-10-03T16:48:40.168865Z"
    }
   },
   "outputs": [],
   "source": [
    "alphas = [0.0, 0.85, 1.0]\n",
    "runs_format = \"{} Q0 {} {} {} DISTILBERT_QL\\n\" #topic_id, doc_id, ranking, score\n",
    "preds = softmax(torch.as_tensor(_preds))[:,0].cpu().numpy()\n",
    "for alpha in alphas:\n",
    "    beta = 1-alpha\n",
    "    out_run_file = os.path.join(\"/ssd2/arthur/TREC2019/data/runs/LNC2_dev_distilBert-{}.run\".format(alpha))\n",
    "    topic_results = [] \n",
    "    last_topic = -1\n",
    "    with open(QL_run_file, 'r') as inf, open(out_run_file, 'w') as outf:\n",
    "        for counter, (example, score) in enumerate(zip(inf, preds)):\n",
    "            topic_id, _, doc_id, _, _, _ = example.split()\n",
    "            guid = \"{}-{}\".format(topic_id, doc_id)\n",
    "            if topic_id != last_topic and len(topic_results) > 0:\n",
    "                topic_results.sort(key=lambda x:x['score'], reverse=True)\n",
    "                for rank, topic in enumerate(topic_results):\n",
    "                    outf.write(runs_format.format(topic['topic_id'], topic['doc_id'], rank, topic['score']))\n",
    "                topic_results = []\n",
    "            topic_results.append({'topic_id':topic_id, 'doc_id':doc_id, 'score':alpha*score + beta*ql_scores[guid]})\n",
    "            last_topic = topic_id\n",
    "        topic_results.sort(key=lambda x:x['score'], reverse=True)\n",
    "        for rank, topic in enumerate(topic_results):\n",
    "            outf.write(runs_format.format(topic['topic_id'], topic['doc_id'], rank, topic['score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T16:52:54.306477Z",
     "start_time": "2019-10-03T16:52:54.301192Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6941"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-03T16:51:14.033361Z",
     "start_time": "2019-10-03T16:51:13.889162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 6941 17 0.002449214810546031\n",
      "0.85 6941 2205 0.31767756807376457\n",
      "1.0 6941 3644 0.5249963982135138\n"
     ]
    }
   ],
   "source": [
    "for alpha in alphas:\n",
    "    LNC2_scores = {}\n",
    "    run_file = \"/ssd2/arthur/TREC2019/data/runs/LNC2_dev_distilBert-{}.run\".format(alpha)\n",
    "    for line in open(run_file):\n",
    "        topic_id, _, doc_id, _, score, _ = line.split()\n",
    "        pair_id = \"{}-{}\".format(topic_id, doc_id)\n",
    "        LNC2_scores[pair_id] = float(score)\n",
    "    dataset_path = os.path.join(args.data_home, \"diagnostics/LNC2-instances\")\n",
    "    dataset = pickle.load(open(dataset_path, 'rb'))\n",
    "    agreements = 0\n",
    "    for topic_id, di_id, dj_id in dataset:\n",
    "        guid1 = f\"{topic_id}-{di_id}\"\n",
    "        guid2 = f\"{topic_id}-{dj_id}\"\n",
    "        if LNC2_scores[guid1] > LNC2_scores[guid2]:\n",
    "            agreements+=1\n",
    "    print(alpha, len(dataset), agreements, agreements / len(dataset))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
