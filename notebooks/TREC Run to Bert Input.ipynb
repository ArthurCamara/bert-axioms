{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T16:24:43.515613Z",
     "start_time": "2019-08-19T16:24:43.511051Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "data_home = \"/ssd2/arthur/TREC2019/data/\"\n",
    "run_file = \"/ssd2/arthur/TREC2019/data/msmarco-doctrain-top100\"\n",
    "assert os.path.isfile(run_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T16:24:55.585520Z",
     "start_time": "2019-08-19T16:24:48.432227Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "import os\n",
    "import gzip\n",
    "import csv\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from pytorch_transformers import BertTokenizer\n",
    "import subprocess\n",
    "split = 'train'\n",
    "top_k = 100\n",
    "\n",
    "queries_file = os.path.join(data_home, f\"msmarco-doc{split}-queries.tsv.gz\")\n",
    "lookup_file = os.path.join(data_home, \"msmarco-docs-lookup.tsv.gz\")\n",
    "qrels_file = os.path.join(data_home, f\"msmarco-doc{split}-qrels.tsv.gz\")\n",
    "docs_file = os.path.join(data_home, \"msmarco-docs.tsv\")\n",
    "output_file = os.path.join(data_home, \"ql_bert_{}_top{}.tsv\".format(split, top_k))\n",
    "\n",
    "n_topics = 0\n",
    "\n",
    "number_of_lines_to_process = int(subprocess.check_output(\"wc -l {}\".format(run_file).split()).decode(\"utf=8\").split()[0])\n",
    "\n",
    "querystring = {}\n",
    "with gzip.open(queries_file, 'rt', encoding='utf-8') as f:\n",
    "    tsvreader = csv.reader(f, delimiter=\"\\t\")\n",
    "    for [topicid, querystring_of_topicid] in tsvreader:\n",
    "        querystring[topicid] = querystring_of_topicid\n",
    "        n_topics +=1\n",
    "        \n",
    "\n",
    "docoffset = {}\n",
    "with gzip.open(lookup_file, 'rt', encoding='utf-8') as f:\n",
    "    tsvreader = csv.reader(f, delimiter=\"\\t\")\n",
    "    for [docid, _, offset] in tsvreader:\n",
    "        docoffset[docid] = int(offset)\n",
    "\n",
    "qrel = {}\n",
    "with gzip.open(qrels_file, 'rt', encoding='utf8') as f:\n",
    "    tsvreader = csv.reader(f, delimiter=\"\\t\")\n",
    "    for [topicid, _, docid, rel] in tsvreader:\n",
    "        assert rel == \"1\"\n",
    "        if topicid in qrel:\n",
    "            qrel[topicid].append(docid)\n",
    "        else:\n",
    "            qrel[topicid] = [docid]\n",
    "\n",
    "def getcontent(docid, file_name):\n",
    "    \"\"\"getcontent(docid, f) will get content for a given docid (a string) from filehandle f.\n",
    "    The content has four tab-separated strings: docid, url, title, body.\n",
    "    \"\"\"\n",
    "    with open(file_name, encoding='utf-8') as f:\n",
    "        f.seek(docoffset[docid])\n",
    "        line = f.readline()\n",
    "        assert line.startswith(docid + \"\\t\"), \\\n",
    "            f\"Looking for {docid}, found {line}\"\n",
    "    return line.rstrip()\n",
    "\n",
    "expected_total = n_topics * top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T16:29:54.558402Z",
     "start_time": "2019-08-19T16:29:54.543909Z"
    }
   },
   "outputs": [],
   "source": [
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length=509):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "\n",
    "\n",
    "\n",
    "def text_to_tokens(query, document, tokenizer):\n",
    "    tokens_a = tokenizer.tokenize(query)\n",
    "    tokens_b = tokenizer.tokenize(document)\n",
    "    _truncate_seq_pair(tokens_a, tokens_b)\n",
    "    tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T16:45:34.855495Z",
     "start_time": "2019-08-19T16:45:34.831709Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mp.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T16:27:58.890163Z",
     "start_time": "2019-08-19T16:27:41.826637Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 60193884\n",
      "1 120416162\n",
      "2 180823912\n",
      "3 241223703\n",
      "4 301567036\n",
      "5 361853175\n",
      "6 422140324\n",
      "7 482440712\n",
      "8 542735323\n",
      "9 602984979\n",
      "10 663223311\n",
      "11 723516992\n",
      "12 783810715\n",
      "13 844096525\n",
      "14 904379955\n",
      "15 964676182\n",
      "16 1024948188\n",
      "17 1084863259\n",
      "18 1144799569\n",
      "19 1204963668\n",
      "20 1265098610\n",
      "21 1325255755\n",
      "22 1385257620\n",
      "23 1445165078\n",
      "24 1505249723\n",
      "25 1565443251\n",
      "26 1625627610\n",
      "27 1685813834\n",
      "28 1746015535\n",
      "29 1806217155\n",
      "30 1866415130\n",
      "31 1926617495\n"
     ]
    }
   ],
   "source": [
    "#pre-process positions for each chunk\n",
    "cpus = 32\n",
    "number_of_chunks = cpus\n",
    "block_offset = dict()\n",
    "lines_per_chunk = number_of_lines_to_process // 32\n",
    "# lines_per_chunk = 5\n",
    "excess_lines = number_of_lines_to_process%32\n",
    "start = 0\n",
    "f = open(run_file)\n",
    "with open(run_file) as f:\n",
    "    current_chunk = 0\n",
    "    counter = 0\n",
    "    line = f.readline()\n",
    "    while(line):\n",
    "        if (counter + 1) % lines_per_chunk == 0:\n",
    "            block_offset[current_chunk] = f.tell()\n",
    "            print(current_chunk, block_offset[current_chunk])\n",
    "            current_chunk+=1\n",
    "        line = f.readline()\n",
    "        \n",
    "        counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T16:31:57.073110Z",
     "start_time": "2019-08-19T16:29:57.054218Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "870ec9acb7254dac88ead6d9424e29ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=32), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65bc804ed7ab4a0f8358986fd2794873",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=32), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-157:\n",
      "Process ForkPoolWorker-130:\n",
      "Process ForkPoolWorker-139:\n",
      "Process ForkPoolWorker-137:\n",
      "Process ForkPoolWorker-138:\n",
      "Process ForkPoolWorker-140:\n",
      "Process ForkPoolWorker-135:\n",
      "Process ForkPoolWorker-148:\n",
      "Process ForkPoolWorker-154:\n",
      "Process ForkPoolWorker-150:\n",
      "Process ForkPoolWorker-136:\n",
      "Process ForkPoolWorker-133:\n",
      "Process ForkPoolWorker-142:\n",
      "Process ForkPoolWorker-147:\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-155:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-68a98138262e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines_per_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mjob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjobs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bert/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bert/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-129:\n",
      "Process ForkPoolWorker-146:\n",
      "Process ForkPoolWorker-143:\n",
      "Process ForkPoolWorker-144:\n",
      "Process ForkPoolWorker-134:\n",
      "Process ForkPoolWorker-149:\n",
      "Process ForkPoolWorker-159:\n",
      "Process ForkPoolWorker-152:\n",
      "Process ForkPoolWorker-151:\n",
      "Process ForkPoolWorker-141:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-156:\n",
      "Process ForkPoolWorker-132:\n",
      "Process ForkPoolWorker-158:\n",
      "Process ForkPoolWorker-145:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-131:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkPoolWorker-160:\n",
      "Process ForkPoolWorker-153:\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-11-68a98138262e>\", line 17, in process_chunk\n",
      "    tokenized = text_to_tokens(query, document, tokenizer)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-11-68a98138262e>\", line 17, in process_chunk\n",
      "    tokenized = text_to_tokens(query, document, tokenizer)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-11-68a98138262e>\", line 17, in process_chunk\n",
      "    tokenized = text_to_tokens(query, document, tokenizer)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-11-68a98138262e>\", line 17, in process_chunk\n",
      "    tokenized = text_to_tokens(query, document, tokenizer)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-10-c005b547fc13>\", line 16, in text_to_tokens\n",
      "    tokens_b = tokenizer.tokenize(document)\n",
      "  File \"<ipython-input-11-68a98138262e>\", line 17, in process_chunk\n",
      "    tokenized = text_to_tokens(query, document, tokenizer)\n",
      "  File \"<ipython-input-11-68a98138262e>\", line 17, in process_chunk\n",
      "    tokenized = text_to_tokens(query, document, tokenizer)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-11-68a98138262e>\", line 17, in process_chunk\n",
      "    tokenized = text_to_tokens(query, document, tokenizer)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-11-68a98138262e>\", line 17, in process_chunk\n",
      "    tokenized = text_to_tokens(query, document, tokenizer)\n",
      "  File \"<ipython-input-11-68a98138262e>\", line 17, in process_chunk\n",
      "    tokenized = text_to_tokens(query, document, tokenizer)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-11-68a98138262e>\", line 17, in process_chunk\n",
      "    tokenized = text_to_tokens(query, document, tokenizer)\n",
      "  File \"<ipython-input-11-68a98138262e>\", line 17, in process_chunk\n",
      "    tokenized = text_to_tokens(query, document, tokenizer)\n",
      "  File \"<ipython-input-11-68a98138262e>\", line 17, in process_chunk\n",
      "    tokenized = text_to_tokens(query, document, tokenizer)\n",
      "  File \"<ipython-input-11-68a98138262e>\", line 17, in process_chunk\n",
      "    tokenized = text_to_tokens(query, document, tokenizer)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-11-68a98138262e>\", line 17, in process_chunk\n",
      "    tokenized = text_to_tokens(query, document, tokenizer)\n",
      "  File \"<ipython-input-11-68a98138262e>\", line 17, in process_chunk\n",
      "    tokenized = text_to_tokens(query, document, tokenizer)\n",
      "  File \"<ipython-input-11-68a98138262e>\", line 17, in process_chunk\n",
      "    tokenized = text_to_tokens(query, document, tokenizer)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-11-68a98138262e>\", line 17, in process_chunk\n",
      "    tokenized = text_to_tokens(query, document, tokenizer)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-11-68a98138262e>\", line 17, in process_chunk\n",
      "    tokenized = text_to_tokens(query, document, tokenizer)\n",
      "  File \"<ipython-input-11-68a98138262e>\", line 17, in process_chunk\n",
      "    tokenized = text_to_tokens(query, document, tokenizer)\n",
      "  File \"<ipython-input-11-68a98138262e>\", line 17, in process_chunk\n",
      "    tokenized = text_to_tokens(query, document, tokenizer)\n",
      "  File \"<ipython-input-11-68a98138262e>\", line 17, in process_chunk\n",
      "    tokenized = text_to_tokens(query, document, tokenizer)\n",
      "  File \"<ipython-input-10-c005b547fc13>\", line 16, in text_to_tokens\n",
      "    tokens_b = tokenizer.tokenize(document)\n",
      "  File \"<ipython-input-11-68a98138262e>\", line 17, in process_chunk\n",
      "    tokenized = text_to_tokens(query, document, tokenizer)\n",
      "  File \"<ipython-input-10-c005b547fc13>\", line 16, in text_to_tokens\n",
      "    tokens_b = tokenizer.tokenize(document)\n",
      "  File \"<ipython-input-11-68a98138262e>\", line 17, in process_chunk\n",
      "    tokenized = text_to_tokens(query, document, tokenizer)\n",
      "  File \"<ipython-input-11-68a98138262e>\", line 17, in process_chunk\n",
      "    tokenized = text_to_tokens(query, document, tokenizer)\n",
      "  File \"<ipython-input-10-c005b547fc13>\", line 16, in text_to_tokens\n",
      "    tokens_b = tokenizer.tokenize(document)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 361, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"<ipython-input-10-c005b547fc13>\", line 15, in text_to_tokens\n",
      "    tokens_a = tokenizer.tokenize(query)\n",
      "  File \"<ipython-input-10-c005b547fc13>\", line 16, in text_to_tokens\n",
      "    tokens_b = tokenizer.tokenize(document)\n",
      "  File \"<ipython-input-11-68a98138262e>\", line 17, in process_chunk\n",
      "    tokenized = text_to_tokens(query, document, tokenizer)\n",
      "  File \"<ipython-input-10-c005b547fc13>\", line 16, in text_to_tokens\n",
      "    tokens_b = tokenizer.tokenize(document)\n",
      "  File \"<ipython-input-10-c005b547fc13>\", line 16, in text_to_tokens\n",
      "    tokens_b = tokenizer.tokenize(document)\n",
      "  File \"<ipython-input-11-68a98138262e>\", line 17, in process_chunk\n",
      "    tokenized = text_to_tokens(query, document, tokenizer)\n",
      "  File \"<ipython-input-10-c005b547fc13>\", line 17, in text_to_tokens\n",
      "    _truncate_seq_pair(tokens_a, tokens_b)\n",
      "  File \"<ipython-input-11-68a98138262e>\", line 17, in process_chunk\n",
      "    tokenized = text_to_tokens(query, document, tokenizer)\n",
      "  File \"<ipython-input-10-c005b547fc13>\", line 16, in text_to_tokens\n",
      "    tokens_b = tokenizer.tokenize(document)\n",
      "  File \"<ipython-input-10-c005b547fc13>\", line 16, in text_to_tokens\n",
      "    tokens_b = tokenizer.tokenize(document)\n",
      "  File \"<ipython-input-10-c005b547fc13>\", line 16, in text_to_tokens\n",
      "    tokens_b = tokenizer.tokenize(document)\n",
      "  File \"<ipython-input-10-c005b547fc13>\", line 16, in text_to_tokens\n",
      "    tokens_b = tokenizer.tokenize(document)\n",
      "  File \"<ipython-input-11-68a98138262e>\", line 16, in process_chunk\n",
      "    document = getcontent(doc_id, docs_file)\n",
      "  File \"<ipython-input-11-68a98138262e>\", line 17, in process_chunk\n",
      "    tokenized = text_to_tokens(query, document, tokenizer)\n",
      "  File \"<ipython-input-10-c005b547fc13>\", line 16, in text_to_tokens\n",
      "    tokens_b = tokenizer.tokenize(document)\n",
      "  File \"<ipython-input-10-c005b547fc13>\", line 16, in text_to_tokens\n",
      "    tokens_b = tokenizer.tokenize(document)\n",
      "  File \"<ipython-input-10-c005b547fc13>\", line 16, in text_to_tokens\n",
      "    tokens_b = tokenizer.tokenize(document)\n",
      "  File \"<ipython-input-10-c005b547fc13>\", line 16, in text_to_tokens\n",
      "    tokens_b = tokenizer.tokenize(document)\n",
      "  File \"<ipython-input-11-68a98138262e>\", line 17, in process_chunk\n",
      "    tokenized = text_to_tokens(query, document, tokenizer)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-10-c005b547fc13>\", line 16, in text_to_tokens\n",
      "    tokens_b = tokenizer.tokenize(document)\n",
      "  File \"<ipython-input-10-c005b547fc13>\", line 16, in text_to_tokens\n",
      "    tokens_b = tokenizer.tokenize(document)\n",
      "  File \"<ipython-input-10-c005b547fc13>\", line 16, in text_to_tokens\n",
      "    tokens_b = tokenizer.tokenize(document)\n",
      "  File \"<ipython-input-10-c005b547fc13>\", line 17, in text_to_tokens\n",
      "    _truncate_seq_pair(tokens_a, tokens_b)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 361, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"<ipython-input-10-c005b547fc13>\", line 16, in text_to_tokens\n",
      "    tokens_b = tokenizer.tokenize(document)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 361, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"<ipython-input-10-c005b547fc13>\", line 16, in text_to_tokens\n",
      "    tokens_b = tokenizer.tokenize(document)\n",
      "  File \"<ipython-input-10-c005b547fc13>\", line 16, in text_to_tokens\n",
      "    tokens_b = tokenizer.tokenize(document)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 361, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 360, in tokenize\n",
      "    added_tokens = list(self.added_tokens_encoder.keys()) + self.all_special_tokens\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 361, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"<ipython-input-10-c005b547fc13>\", line 16, in text_to_tokens\n",
      "    tokens_b = tokenizer.tokenize(document)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 361, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 361, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"<ipython-input-10-c005b547fc13>\", line 7, in _truncate_seq_pair\n",
      "    if len(tokens_a) > len(tokens_b):\n",
      "  File \"<ipython-input-10-c005b547fc13>\", line 16, in text_to_tokens\n",
      "    tokens_b = tokenizer.tokenize(document)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 361, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"<ipython-input-10-c005b547fc13>\", line 16, in text_to_tokens\n",
      "    tokens_b = tokenizer.tokenize(document)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 361, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 361, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"<ipython-input-10-c005b547fc13>\", line 16, in text_to_tokens\n",
      "    tokens_b = tokenizer.tokenize(document)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 361, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"<ipython-input-4-4dcea884cf5c>\", line 52, in getcontent\n",
      "    line = f.readline()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 361, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 361, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 361, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 361, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"<ipython-input-10-c005b547fc13>\", line 16, in text_to_tokens\n",
      "    tokens_b = tokenizer.tokenize(document)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/queues.py\", line 352, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"<ipython-input-11-68a98138262e>\", line 17, in process_chunk\n",
      "    tokenized = text_to_tokens(query, document, tokenizer)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 361, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 361, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 361, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"<ipython-input-10-c005b547fc13>\", line 7, in _truncate_seq_pair\n",
      "    if len(tokens_a) > len(tokens_b):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 361, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 361, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 361, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 468, in all_special_tokens\n",
      "    all_toks = list(set(all_toks))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 361, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 361, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 361, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 361, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/codecs.py\", line 319, in decode\n",
      "    def decode(self, input, final=False):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 361, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"<ipython-input-10-c005b547fc13>\", line 16, in text_to_tokens\n",
      "    tokens_b = tokenizer.tokenize(document)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 361, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 354, in split_on_tokens\n",
      "    return self._tokenize(text, **kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 354, in split_on_tokens\n",
      "    return self._tokenize(text, **kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 354, in split_on_tokens\n",
      "    return self._tokenize(text, **kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 354, in split_on_tokens\n",
      "    return self._tokenize(text, **kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 354, in split_on_tokens\n",
      "    return self._tokenize(text, **kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 354, in split_on_tokens\n",
      "    return self._tokenize(text, **kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 354, in split_on_tokens\n",
      "    return self._tokenize(text, **kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 354, in split_on_tokens\n",
      "    return self._tokenize(text, **kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 354, in split_on_tokens\n",
      "    return self._tokenize(text, **kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 354, in split_on_tokens\n",
      "    return self._tokenize(text, **kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 354, in split_on_tokens\n",
      "    return self._tokenize(text, **kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 354, in split_on_tokens\n",
      "    return self._tokenize(text, **kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 354, in split_on_tokens\n",
      "    return self._tokenize(text, **kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 354, in split_on_tokens\n",
      "    return self._tokenize(text, **kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 354, in split_on_tokens\n",
      "    return self._tokenize(text, **kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 149, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 149, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 354, in split_on_tokens\n",
      "    return self._tokenize(text, **kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 354, in split_on_tokens\n",
      "    return self._tokenize(text, **kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 150, in _tokenize\n",
      "    for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 149, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 149, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 354, in split_on_tokens\n",
      "    return self._tokenize(text, **kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 149, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 149, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 149, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 354, in split_on_tokens\n",
      "    return self._tokenize(text, **kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 149, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 149, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 354, in split_on_tokens\n",
      "    return self._tokenize(text, **kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 354, in split_on_tokens\n",
      "    return self._tokenize(text, **kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 149, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 149, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 149, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 149, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 354, in split_on_tokens\n",
      "    return self._tokenize(text, **kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 354, in split_on_tokens\n",
      "    return self._tokenize(text, **kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 354, in split_on_tokens\n",
      "    return self._tokenize(text, **kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 149, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 245, in tokenize\n",
      "    text = self._tokenize_chinese_chars(text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 252, in tokenize\n",
      "    split_tokens.extend(self._run_split_on_punc(token))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 149, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 149, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 379, in tokenize\n",
      "    substr = \"\".join(chars[start:end])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 252, in tokenize\n",
      "    split_tokens.extend(self._run_split_on_punc(token))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 149, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 252, in tokenize\n",
      "    split_tokens.extend(self._run_split_on_punc(token))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 237, in tokenize\n",
      "    text = self._clean_text(text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 251, in tokenize\n",
      "    token = self._run_strip_accents(token)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 237, in tokenize\n",
      "    text = self._clean_text(text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 150, in _tokenize\n",
      "    for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 245, in tokenize\n",
      "    text = self._tokenize_chinese_chars(text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 237, in tokenize\n",
      "    text = self._clean_text(text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 149, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 354, in split_on_tokens\n",
      "    return self._tokenize(text, **kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 252, in tokenize\n",
      "    split_tokens.extend(self._run_split_on_punc(token))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 149, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 252, in tokenize\n",
      "    split_tokens.extend(self._run_split_on_punc(token))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 237, in tokenize\n",
      "    text = self._clean_text(text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 252, in tokenize\n",
      "    split_tokens.extend(self._run_split_on_punc(token))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 150, in _tokenize\n",
      "    for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 149, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 150, in _tokenize\n",
      "    for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 237, in tokenize\n",
      "    text = self._clean_text(text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 301, in _tokenize_chinese_chars\n",
      "    return \"\".join(output)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 252, in tokenize\n",
      "    split_tokens.extend(self._run_split_on_punc(token))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 285, in _run_split_on_punc\n",
      "    output[-1].append(char)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 237, in tokenize\n",
      "    text = self._clean_text(text)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 354, in split_on_tokens\n",
      "    return self._tokenize(text, **kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 252, in tokenize\n",
      "    split_tokens.extend(self._run_split_on_punc(token))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 277, in _run_split_on_punc\n",
      "    char = chars[i]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 278, in _run_split_on_punc\n",
      "    if _is_punctuation(char):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 330, in _clean_text\n",
      "    if cp == 0 or cp == 0xfffd or _is_control(char):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 261, in _run_strip_accents\n",
      "    for char in text:\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 329, in _clean_text\n",
      "    cp = ord(char)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 376, in tokenize\n",
      "    end = len(chars)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 295, in _tokenize_chinese_chars\n",
      "    if self._is_chinese_char(cp):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 330, in _clean_text\n",
      "    if cp == 0 or cp == 0xfffd or _is_control(char):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 237, in tokenize\n",
      "    text = self._clean_text(text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 149, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 288, in _run_split_on_punc\n",
      "    return [\"\".join(x) for x in output]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 237, in tokenize\n",
      "    text = self._clean_text(text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 278, in _run_split_on_punc\n",
      "    if _is_punctuation(char):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 332, in _clean_text\n",
      "    if _is_whitespace(char):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 288, in _run_split_on_punc\n",
      "    return [\"\".join(x) for x in output]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 237, in tokenize\n",
      "    text = self._clean_text(text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 382, in tokenize\n",
      "    if substr in self.vocab:\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 379, in tokenize\n",
      "    substr = \"\".join(chars[start:end])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 335, in _clean_text\n",
      "    output.append(char)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 278, in _run_split_on_punc\n",
      "    if _is_punctuation(char):\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 149, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 278, in _run_split_on_punc\n",
      "    if _is_punctuation(char):\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 329, in _clean_text\n",
      "    cp = ord(char)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 430, in _is_punctuation\n",
      "    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in split_on_tokens\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 317, in _is_chinese_char\n",
      "    (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 417, in _is_control\n",
      "    cat = unicodedata.category(char)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 330, in _clean_text\n",
      "    if cp == 0 or cp == 0xfffd or _is_control(char):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 245, in tokenize\n",
      "    text = self._tokenize_chinese_chars(text)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 330, in _clean_text\n",
      "    if cp == 0 or cp == 0xfffd or _is_control(char):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 288, in <listcomp>\n",
      "    return [\"\".join(x) for x in output]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 403, in _is_whitespace\n",
      "    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 430, in _is_punctuation\n",
      "    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 330, in _clean_text\n",
      "    if cp == 0 or cp == 0xfffd or _is_control(char):\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 434, in _is_punctuation\n",
      "    if cat.startswith(\"P\"):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 237, in tokenize\n",
      "    text = self._clean_text(text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 431, in _is_punctuation\n",
      "    (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 358, in <genexpr>\n",
      "    for sub_text in split_text), [])[:-1]\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 295, in _tokenize_chinese_chars\n",
      "    if self._is_chinese_char(cp):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 418, in _is_control\n",
      "    if cat.startswith(\"C\"):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 418, in _is_control\n",
      "    if cat.startswith(\"C\"):\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 335, in _clean_text\n",
      "    output.append(char)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 354, in split_on_tokens\n",
      "    return self._tokenize(text, **kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 319, in _is_chinese_char\n",
      "    (cp >= 0xF900 and cp <= 0xFAFF) or  #\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 149, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 252, in tokenize\n",
      "    split_tokens.extend(self._run_split_on_punc(token))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 278, in _run_split_on_punc\n",
      "    if _is_punctuation(char):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 430, in _is_punctuation\n",
      "    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp, os\n",
    "\n",
    "def process_chunk(chunk_no, block_offset, inf, no_lines):\n",
    "    lines = []\n",
    "    with open(inf, 'r') as f:\n",
    "        f.seek(block_offset[chunk_no])\n",
    "        for i in range(no_lines):\n",
    "            lines.append(f.readline().strip())\n",
    "    tokenizer = BertTokenizer.from_pretrained(os.path.join(data_home, \"models\"))\n",
    "    output_line_format = \"{}-{}\\t{}\\t{}\\n\"\n",
    "    with open(\"/ssd2/arthur/TREC2019/data/{}-triples.{}\".format(split, chunk_no), 'w', encoding='utf-8') as outf:\n",
    "        for line in tqdm(lines):\n",
    "            [topic_id, _, doc_id, ranking, score, _] = line.split()\n",
    "            is_relevant = doc_id in qrel[topic_id]\n",
    "            query = querystring[topic_id]\n",
    "            document = getcontent(doc_id, docs_file)\n",
    "            tokenized = text_to_tokens(query, document, tokenizer)\n",
    "            outf.write(output_line_format.format(topic_id, doc_id, tokenized, int(is_relevant)))\n",
    "\n",
    "\n",
    "pbar = tqdm(total=32)\n",
    "def update(*a):\n",
    "    pbar.update()\n",
    "pool = mp.Pool(32)\n",
    "jobs = []\n",
    "for i in tqdm(range(32)):\n",
    "    jobs.append(pool.apply_async(process_chunk, args=(i, block_offset, run_file, lines_per_chunk, ),callback=update ))\n",
    "for job in jobs:\n",
    "    job.get()\n",
    "\n",
    "pool.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T14:51:39.665434Z",
     "start_time": "2019-08-19T14:51:38.256069Z"
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp,os\n",
    "number_of_lines_to_process = int(subprocess.check_output(\"wc -l {}\".format(run_file).split()).decode(\"utf=8\").split()[0])\n",
    "run_file = \"/ssd2/arthur/TREC2019/data/msmarco-doctrain-top100\"\n",
    "\n",
    "def process_wrapper(chunkStart, chunkSize, chunk_no):\n",
    "    with open(\"input.txt\") as f, open(\"output_{}.txt\".format(chunk_no)) as outf:\n",
    "        f.seek(chunkStart)\n",
    "        lines = f.read(chunkSize).splitlines()\n",
    "        for line in lines:\n",
    "            [topic_id, _, doc_id, ranking, score, _] = line.split()\n",
    "            is_relevant = doc_id in qrel[topic_id]\n",
    "            query = querystring[topic_id]\n",
    "            document = getcontent(doc_id, f)\n",
    "            tokenized = text_to_tokens(query, document)\n",
    "            outf.write(output_line_format.format(topic_id, doc_id, tokenized, int(is_relevant)))\n",
    "\n",
    "def chunkify(fname,size=1024*1024):\n",
    "    fileEnd = os.path.getsize(fname)\n",
    "    print(fileEnd)\n",
    "    with open(fname,'r') as f:\n",
    "        chunkEnd = f.tell()\n",
    "        while True:\n",
    "            chunkStart = chunkEnd\n",
    "            f.seek(size,1)\n",
    "            f.readline()\n",
    "            chunkEnd = f.tell()\n",
    "            yield chunkStart, chunkEnd - chunkStart\n",
    "            if chunkEnd > fileEnd:\n",
    "                break\n",
    "\n",
    "# pool = mp.Pool(32)\n",
    "# jobs = []\n",
    "for n_chunk, chunkStart, chunkSize in enumerate(chunkify(run_file)):\n",
    "    print(n_chunk)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T14:31:01.746686Z",
     "start_time": "2019-08-19T14:31:01.739359Z"
    }
   },
   "outputs": [],
   "source": [
    "36699977//32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T14:30:30.526657Z",
     "start_time": "2019-08-19T14:30:30.520342Z"
    }
   },
   "outputs": [],
   "source": [
    "number_of_lines_to_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T13:56:51.246900Z",
     "start_time": "2019-08-19T13:56:47.635486Z"
    }
   },
   "outputs": [],
   "source": [
    "from multiprocessing import os\n",
    "\n",
    "def process_chunk()\n",
    "\n",
    "output_line_format = \"{}-{}\\t{}\\t{}\\n\" #query_id, tokenized_text, label\n",
    "with open(run_file, 'r') as inf, open(docs_file, 'r') as f, open(output_file, 'w', encoding='utf-8') as outf:\n",
    "    for counter, line in tqdm(enumerate(inf), total=expected_total):\n",
    "        [topic_id, _, doc_id, ranking, score, _] = line.split()\n",
    "        is_relevant = doc_id in qrel[topic_id]\n",
    "        \n",
    "        query = querystring[topic_id]\n",
    "        document = getcontent(doc_id, f)\n",
    "        tokenized = text_to_tokens(query, document)\n",
    "        outf.write(output_line_format.format(topic_id, doc_id, tokenized, int(is_relevant)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
