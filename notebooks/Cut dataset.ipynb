{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T08:21:06.620442Z",
     "start_time": "2019-09-23T08:20:31.539948Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d0e610ce46f42a8a236508c70afe3b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "tokenized_dataset_path = \"/ssd2/arthur/TREC2019/data/docs/tokenized-msmarco-docs.tsv\"\n",
    "trectext_format=\"<DOC>\\n<DOCNO>{}</DOCNO>\\n<TEXT>\\n{}\\n</TEXT>\\n</DOC>\\n\"\n",
    "with open(\"/ssd2/arthur/TREC2019/data/docs/tokenized-msmarco-docs.trec\", 'w') as outf:    \n",
    "    for line in tqdm(open(tokenized_dataset_path)):\n",
    "        docno = line.split(\"\\t\")[0]\n",
    "        doc_text = \" \".join(line.split(\"\\t\")[1:])\n",
    "        outf.write(trectext_format.format(docno,doc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T23:48:34.107673Z",
     "start_time": "2019-10-09T23:48:34.100477Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D1555982',\n",
       " 'https://answers.yahoo.com/question/index?qid=20071007114826AAwCFvR',\n",
       " 'The hot glowing surfaces of stars emit energy in the form of electromagnetic radiation.?',\n",
       " 'Science & Mathematics PhysicsThe hot glowing surfaces of stars emit energy in the form of electromagnetic radiation.?It is a good approximation to assume that the emissivity e is equal to 1 for these surfaces.  Find the radius of the star Rigel, the bright blue star in the constellation Orion that radiates energy at a rate of 2.7 x 10^32 W and has a surface temperature of 11,000 K. Assume that the star is spherical. Use Ï\\x83 =... show moreFollow 3 answersAnswersRelevanceRatingNewestOldestBest Answer: Stefan-Boltzmann law states that the energy flux by radiation is proportional to the forth power of the temperature: q = Îµ Â· Ï\\x83 Â· T^4 The total energy flux at a spherical surface of Radius R is Q = qÂ·Ï\\x80Â·RÂ² = ÎµÂ·Ï\\x83Â·T^4Â·Ï\\x80Â·RÂ² Hence the radius is R = â\\x88\\x9a ( Q / (ÎµÂ·Ï\\x83Â·T^4Â·Ï\\x80) ) = â\\x88\\x9a ( 2.7x10+32 W / (1 Â· 5.67x10-8W/mÂ²K^4 Â· (1100K)^4 Â· Ï\\x80) ) = 3.22x10+13 mSource (s):http://en.wikipedia.org/wiki/Stefan_bolt...schmiso Â· 1 decade ago0 18 CommentSchmiso, you forgot a 4 in your answer. Your link even says it: L = 4pi (R^2)sigma (T^4). Using L, luminosity, as the energy in this problem, you can find the radius R by doing sqrt (L/ (4pisigma (T^4)). Hope this helps everyone.Caroline Â· 4 years ago4 1 Comment (Stefan-Boltzmann law) L = 4pi*R^2*sigma*T^4 Solving for R we get: => R = (1/ (2T^2)) * sqrt (L/ (pi*sigma)) Plugging in your values you should get: => R = (1/ (2 (11,000K)^2)) *sqrt ( (2.7*10^32W)/ (pi * (5.67*10^-8 W/m^2K^4))) R = 1.609 * 10^11 m? Â· 3 years ago0 1 CommentMaybe you would like to learn more about one of these?Want to build a free website? Interested in dating sites?Need a Home Security Safe? How to order contacts online?\\n']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line.decode().split(\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-10-09T23:48:58.883Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3b4b7e9325d4cd3ac1cb615e8c6739d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=3213835), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "offset_dict = {}\n",
    "with open(\"/ssd2/arthur/TREC2019/data/docs/msmarco-docs.tsv\") as inf:\n",
    "    location = inf.tell()\n",
    "    line = inf.readline().encode(\"utf-8\")\n",
    "    pbar = tqdm(total = 3213835)\n",
    "    idx = 0 \n",
    "    while line:\n",
    "        did = line.decode().split(\"\\t\")[0]\n",
    "        offset_dict[did] = location\n",
    "        location = inf.tell()\n",
    "        line = inf.readline().encode(\"utf-8\")\n",
    "        pbar.update()\n",
    "        idx+=1\n",
    "    pbar.close()\n",
    "import pickle\n",
    "pickle.dump(offset_dict, open(\"/ssd2/arthur/TREC2019/data/docs/msmarco-docs.tsv.offset\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T16:53:55.642410Z",
     "start_time": "2019-09-20T16:53:53.802235Z"
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T20:44:48.956284Z",
     "start_time": "2019-09-23T20:44:48.600977Z"
    }
   },
   "outputs": [],
   "source": [
    "#TSV to TREC \n",
    "import re\n",
    "#tokenize in the same way as the documents!!\n",
    "queries_file = \"/ssd2/arthur/TREC2019/data/queries/test_queries.tsv\"\n",
    "output_file = \"/ssd2/arthur/TREC2019/data/queries/test_queries.params\"\n",
    "param_format = \"<query>\\n<number>{}</number>\\n<text>#combine( {} )</text>\\n</query>\\n\"\n",
    "with open(queries_file) as inf, open(output_file, 'w') as outf:\n",
    "    outf.write(\"<parameters>\\n\")\n",
    "    for line in inf:\n",
    "        query_id = line.split(\"\\t\")[0]\n",
    "        query = tokenizer.tokenize(line.split(\"\\t\")[1])\n",
    "        query = ' '.join([x for x in query]).replace(\"##\", \"\")\n",
    "        query = re.sub(r'([^\\s\\w]|_)+', '', query)\n",
    "        outf.write(param_format.format(query_id, query))\n",
    "    outf.write(\"</parameters>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T16:52:18.110620Z",
     "start_time": "2019-09-20T16:52:18.102865Z"
    }
   },
   "outputs": [],
   "source": [
    "def truncate_seq_pair(tokens_a, tokens_b, max_length=509):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T17:09:35.330103Z",
     "start_time": "2019-09-20T17:08:55.393851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72daea7b23e94fe49d830a3419f8b57a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=36701300), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "split = \"train\"\n",
    "all_docs = []\n",
    "with open(\"/ssd2/arthur/TREC2019/data/triples-tokenized/cut-{}.top100\".format(split), 'w') as outf:\n",
    "    for line in tqdm(open(run_file), total = 100*len(queries)):\n",
    "        topic_id, _, doc_id, _, _, _ = line.split()\n",
    "        all_docs.append((topic_id, doc_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T17:17:06.897619Z",
     "start_time": "2019-09-20T17:17:06.891654Z"
    }
   },
   "outputs": [],
   "source": [
    "n_cpus = 36\n",
    "excess_lines = len(all_docs) % n_cpus\n",
    "if excess_lines > 0:\n",
    "    number_of_chunks = n_cpus - 1\n",
    "\n",
    "else:\n",
    "    number_of_chunks = n_cpus\n",
    "chunk_size = len(all_docs)//number_of_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-20T17:46:07.072467Z",
     "start_time": "2019-09-20T17:44:50.271847Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c5b4089f08452f9805cc78378a1d44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=35), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-265:\n",
      "Process ForkPoolWorker-244:\n",
      "Process ForkPoolWorker-262:\n",
      "Process ForkPoolWorker-248:\n",
      "Process ForkPoolWorker-260:\n",
      "Process ForkPoolWorker-251:\n",
      "Process ForkPoolWorker-257:\n",
      "Process ForkPoolWorker-253:\n",
      "Process ForkPoolWorker-254:\n",
      "Process ForkPoolWorker-240:\n",
      "Process ForkPoolWorker-238:\n",
      "Process ForkPoolWorker-258:\n",
      "Process ForkPoolWorker-246:\n",
      "Process ForkPoolWorker-243:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Process ForkPoolWorker-268:\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-288:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n",
      "    with self._rlock:\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-0ffb58724393>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mjob\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjobs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bert/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bert/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n",
      "    task = get()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/queues.py\", line 352, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-255:\n",
      "Process ForkPoolWorker-264:\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-256:\n",
      "Process ForkPoolWorker-276:\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "Process ForkPoolWorker-285:\n",
      "Process ForkPoolWorker-247:\n",
      "Process ForkPoolWorker-280:\n",
      "Process ForkPoolWorker-266:\n",
      "Process ForkPoolWorker-261:\n",
      "Process ForkPoolWorker-274:\n",
      "Process ForkPoolWorker-249:\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-286:\n",
      "Process ForkPoolWorker-270:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-250:\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process ForkPoolWorker-259:\n",
      "Process ForkPoolWorker-282:\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-53-208ac9ba460e>\", line 13, in process_chunk\n",
      "    doc_content = tokenizer.tokenize(getcontent(doc_id, tokenized_docs))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-53-208ac9ba460e>\", line 13, in process_chunk\n",
      "    doc_content = tokenizer.tokenize(getcontent(doc_id, tokenized_docs))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process ForkPoolWorker-287:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-53-208ac9ba460e>\", line 13, in process_chunk\n",
      "    doc_content = tokenizer.tokenize(getcontent(doc_id, tokenized_docs))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-53-208ac9ba460e>\", line 13, in process_chunk\n",
      "    doc_content = tokenizer.tokenize(getcontent(doc_id, tokenized_docs))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 640, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-273:\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 640, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-53-208ac9ba460e>\", line 13, in process_chunk\n",
      "    doc_content = tokenizer.tokenize(getcontent(doc_id, tokenized_docs))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 640, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-53-208ac9ba460e>\", line 13, in process_chunk\n",
      "    doc_content = tokenizer.tokenize(getcontent(doc_id, tokenized_docs))\n",
      "Process ForkPoolWorker-278:\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 640, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"<ipython-input-53-208ac9ba460e>\", line 13, in process_chunk\n",
      "    doc_content = tokenizer.tokenize(getcontent(doc_id, tokenized_docs))\n",
      "Process ForkPoolWorker-283:\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in split_on_tokens\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Process ForkPoolWorker-275:\n",
      "Process ForkPoolWorker-271:\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in split_on_tokens\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"<ipython-input-53-208ac9ba460e>\", line 13, in process_chunk\n",
      "    doc_content = tokenizer.tokenize(getcontent(doc_id, tokenized_docs))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-53-208ac9ba460e>\", line 13, in process_chunk\n",
      "    doc_content = tokenizer.tokenize(getcontent(doc_id, tokenized_docs))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in split_on_tokens\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-53-208ac9ba460e>\", line 13, in process_chunk\n",
      "    doc_content = tokenizer.tokenize(getcontent(doc_id, tokenized_docs))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 640, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 640, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 640, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in split_on_tokens\n",
      "    else [token] for token in tokenized_text), [])\n",
      "Process ForkPoolWorker-245:\n",
      "Process ForkPoolWorker-269:\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in <genexpr>\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 640, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 640, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in <genexpr>\n",
      "    else [token] for token in tokenized_text), [])\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Process ForkPoolWorker-241:\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in <genexpr>\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"<ipython-input-53-208ac9ba460e>\", line 13, in process_chunk\n",
      "    doc_content = tokenizer.tokenize(getcontent(doc_id, tokenized_docs))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in split_on_tokens\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 640, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in split_on_tokens\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in <genexpr>\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in split_on_tokens\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 170, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "Process ForkPoolWorker-267:\n",
      "Process ForkPoolWorker-281:\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-53-208ac9ba460e>\", line 13, in process_chunk\n",
      "    doc_content = tokenizer.tokenize(getcontent(doc_id, tokenized_docs))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 170, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in split_on_tokens\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"<ipython-input-53-208ac9ba460e>\", line 13, in process_chunk\n",
      "    doc_content = tokenizer.tokenize(getcontent(doc_id, tokenized_docs))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in split_on_tokens\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in split_on_tokens\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 640, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "Process ForkPoolWorker-279:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 170, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in <genexpr>\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in <genexpr>\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in <genexpr>\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 170, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 258, in tokenize\n",
      "    text = self._clean_text(text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 640, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 273, in tokenize\n",
      "    split_tokens.extend(self._run_split_on_punc(token))\n",
      "  File \"<ipython-input-53-208ac9ba460e>\", line 13, in process_chunk\n",
      "    doc_content = tokenizer.tokenize(getcontent(doc_id, tokenized_docs))\n",
      "  File \"<ipython-input-53-208ac9ba460e>\", line 13, in process_chunk\n",
      "    doc_content = tokenizer.tokenize(getcontent(doc_id, tokenized_docs))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 640, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in <genexpr>\n",
      "    else [token] for token in tokenized_text), [])\n",
      "Process ForkPoolWorker-242:\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-53-208ac9ba460e>\", line 13, in process_chunk\n",
      "    doc_content = tokenizer.tokenize(getcontent(doc_id, tokenized_docs))\n",
      "  File \"<ipython-input-53-208ac9ba460e>\", line 13, in process_chunk\n",
      "    doc_content = tokenizer.tokenize(getcontent(doc_id, tokenized_docs))\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "Process ForkPoolWorker-263:\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in <genexpr>\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in <genexpr>\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in split_on_tokens\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 258, in tokenize\n",
      "    text = self._clean_text(text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 170, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 170, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 170, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 273, in tokenize\n",
      "    split_tokens.extend(self._run_split_on_punc(token))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 351, in _clean_text\n",
      "    if cp == 0 or cp == 0xfffd or _is_control(char):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in split_on_tokens\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 640, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 299, in _run_split_on_punc\n",
      "    if _is_punctuation(char):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 640, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in split_on_tokens\n",
      "    else [token] for token in tokenized_text), [])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 170, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 640, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 640, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "Process ForkPoolWorker-284:\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-53-208ac9ba460e>\", line 13, in process_chunk\n",
      "    doc_content = tokenizer.tokenize(getcontent(doc_id, tokenized_docs))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 170, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "Process ForkPoolWorker-272:\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 170, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in <genexpr>\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 273, in tokenize\n",
      "    split_tokens.extend(self._run_split_on_punc(token))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 353, in _clean_text\n",
      "    if _is_whitespace(char):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 273, in tokenize\n",
      "    split_tokens.extend(self._run_split_on_punc(token))\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 273, in tokenize\n",
      "    split_tokens.extend(self._run_split_on_punc(token))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 296, in _run_split_on_punc\n",
      "    output = []\n",
      "Process ForkPoolWorker-239:\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in <genexpr>\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in split_on_tokens\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 455, in _is_punctuation\n",
      "    if cat.startswith(\"P\"):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in split_on_tokens\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in <genexpr>\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"<ipython-input-53-208ac9ba460e>\", line 13, in process_chunk\n",
      "    doc_content = tokenizer.tokenize(getcontent(doc_id, tokenized_docs))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 272, in tokenize\n",
      "    token = self._run_strip_accents(token)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-53-208ac9ba460e>\", line 13, in process_chunk\n",
      "    doc_content = tokenizer.tokenize(getcontent(doc_id, tokenized_docs))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in split_on_tokens\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in split_on_tokens\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-53-208ac9ba460e>\", line 14, in process_chunk\n",
      "    label = \"1\" if _id in qrels else \"0\"\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 640, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 266, in tokenize\n",
      "    text = self._tokenize_chinese_chars(text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 170, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 258, in tokenize\n",
      "    text = self._clean_text(text)\n",
      "  File \"<ipython-input-53-208ac9ba460e>\", line 13, in process_chunk\n",
      "    doc_content = tokenizer.tokenize(getcontent(doc_id, tokenized_docs))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 424, in _is_whitespace\n",
      "    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 306, in _run_split_on_punc\n",
      "    output[-1].append(char)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 299, in _run_split_on_punc\n",
      "    if _is_punctuation(char):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 293, in _run_split_on_punc\n",
      "    chars = list(text)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in <genexpr>\n",
      "    else [token] for token in tokenized_text), [])\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-252:\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 170, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in <genexpr>\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 170, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 640, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 283, in _run_strip_accents\n",
      "    cat = unicodedata.category(char)\n",
      "Process ForkPoolWorker-277:\n",
      "  File \"<ipython-input-53-208ac9ba460e>\", line 13, in process_chunk\n",
      "    doc_content = tokenizer.tokenize(getcontent(doc_id, tokenized_docs))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 640, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in <genexpr>\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in <genexpr>\n",
      "    else [token] for token in tokenized_text), [])\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"<ipython-input-53-208ac9ba460e>\", line 13, in process_chunk\n",
      "    doc_content = tokenizer.tokenize(getcontent(doc_id, tokenized_docs))\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in split_on_tokens\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 321, in _tokenize_chinese_chars\n",
      "    output.append(char)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 258, in tokenize\n",
      "    text = self._clean_text(text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 351, in _clean_text\n",
      "    if cp == 0 or cp == 0xfffd or _is_control(char):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 640, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-53-208ac9ba460e>\", line 13, in process_chunk\n",
      "    doc_content = tokenizer.tokenize(getcontent(doc_id, tokenized_docs))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 446, in _is_punctuation\n",
      "    cp = ord(char)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 170, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 170, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 272, in tokenize\n",
      "    token = self._run_strip_accents(token)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 273, in tokenize\n",
      "    split_tokens.extend(self._run_split_on_punc(token))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in split_on_tokens\n",
      "    else [token] for token in tokenized_text), [])\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in split_on_tokens\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 170, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 170, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-53-208ac9ba460e>\", line 13, in process_chunk\n",
      "    doc_content = tokenizer.tokenize(getcontent(doc_id, tokenized_docs))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 640, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"<ipython-input-53-208ac9ba460e>\", line 13, in process_chunk\n",
      "    doc_content = tokenizer.tokenize(getcontent(doc_id, tokenized_docs))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 640, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in <genexpr>\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 353, in _clean_text\n",
      "    if _is_whitespace(char):\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 640, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 438, in _is_control\n",
      "    cat = unicodedata.category(char)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in split_on_tokens\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 273, in tokenize\n",
      "    split_tokens.extend(self._run_split_on_punc(token))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 258, in tokenize\n",
      "    text = self._clean_text(text)\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-53-208ac9ba460e>\", line 13, in process_chunk\n",
      "    doc_content = tokenizer.tokenize(getcontent(doc_id, tokenized_docs))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in <genexpr>\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 282, in _run_strip_accents\n",
      "    for char in text:\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in <genexpr>\n",
      "    else [token] for token in tokenized_text), [])\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 266, in tokenize\n",
      "    text = self._tokenize_chinese_chars(text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 640, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in split_on_tokens\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 640, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-53-208ac9ba460e>\", line 13, in process_chunk\n",
      "    doc_content = tokenizer.tokenize(getcontent(doc_id, tokenized_docs))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in <genexpr>\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 258, in tokenize\n",
      "    text = self._clean_text(text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 170, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 351, in _clean_text\n",
      "    if cp == 0 or cp == 0xfffd or _is_control(char):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in split_on_tokens\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 640, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 170, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 170, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in split_on_tokens\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 298, in _run_split_on_punc\n",
      "    char = chars[i]\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 316, in _tokenize_chinese_chars\n",
      "    if self._is_chinese_char(cp):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in split_on_tokens\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in <genexpr>\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/process.py\", line 99, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 640, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 170, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"<ipython-input-53-208ac9ba460e>\", line 13, in process_chunk\n",
      "    doc_content = tokenizer.tokenize(getcontent(doc_id, tokenized_docs))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 353, in _clean_text\n",
      "    if _is_whitespace(char):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 439, in _is_control\n",
      "    if cat.startswith(\"C\"):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in split_on_tokens\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"<ipython-input-53-208ac9ba460e>\", line 13, in process_chunk\n",
      "    doc_content = tokenizer.tokenize(getcontent(doc_id, tokenized_docs))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 424, in _is_whitespace\n",
      "    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in split_on_tokens\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in <genexpr>\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 337, in _is_chinese_char\n",
      "    (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 273, in tokenize\n",
      "    split_tokens.extend(self._run_split_on_punc(token))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in <genexpr>\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 272, in tokenize\n",
      "    token = self._run_strip_accents(token)\n",
      "  File \"<ipython-input-53-208ac9ba460e>\", line 13, in process_chunk\n",
      "    doc_content = tokenizer.tokenize(getcontent(doc_id, tokenized_docs))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in <genexpr>\n",
      "    else [token] for token in tokenized_text), [])\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in split_on_tokens\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in <genexpr>\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 272, in tokenize\n",
      "    token = self._run_strip_accents(token)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 640, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"<ipython-input-53-208ac9ba460e>\", line 13, in process_chunk\n",
      "    doc_content = tokenizer.tokenize(getcontent(doc_id, tokenized_docs))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 170, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 266, in tokenize\n",
      "    text = self._tokenize_chinese_chars(text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 426, in _is_whitespace\n",
      "    cat = unicodedata.category(char)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 640, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in <genexpr>\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 170, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 309, in _run_split_on_punc\n",
      "    return [\"\".join(x) for x in output]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 170, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 280, in _run_strip_accents\n",
      "    text = unicodedata.normalize(\"NFD\", text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 170, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in <genexpr>\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 640, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 283, in _run_strip_accents\n",
      "    cat = unicodedata.category(char)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 170, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in split_on_tokens\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 266, in tokenize\n",
      "    text = self._tokenize_chinese_chars(text)\n",
      "  File \"<ipython-input-38-1a04853d9390>\", line 6, in getcontent\n",
      "    doc = f.readline()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 316, in _tokenize_chinese_chars\n",
      "    if self._is_chinese_char(cp):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in split_on_tokens\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 309, in <listcomp>\n",
      "    return [\"\".join(x) for x in output]\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 273, in tokenize\n",
      "    split_tokens.extend(self._run_split_on_punc(token))\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 316, in _tokenize_chinese_chars\n",
      "    if self._is_chinese_char(cp):\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 170, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 273, in tokenize\n",
      "    split_tokens.extend(self._run_split_on_punc(token))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in <genexpr>\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in <genexpr>\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/encodings/latin_1.py\", line 25, in decode\n",
      "    def decode(self, input, final=False):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 258, in tokenize\n",
      "    text = self._clean_text(text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in split_on_tokens\n",
      "    else [token] for token in tokenized_text), [])\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 258, in tokenize\n",
      "    text = self._clean_text(text)\n",
      "  File \"<ipython-input-53-208ac9ba460e>\", line 13, in process_chunk\n",
      "    doc_content = tokenizer.tokenize(getcontent(doc_id, tokenized_docs))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 334, in _is_chinese_char\n",
      "    if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in <genexpr>\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 170, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 299, in _run_split_on_punc\n",
      "    if _is_punctuation(char):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 640, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 170, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 171, in _tokenize\n",
      "    for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 452, in _is_punctuation\n",
      "    (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 258, in tokenize\n",
      "    text = self._clean_text(text)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in split_on_tokens\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 353, in _clean_text\n",
      "    if _is_whitespace(char):\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 426, in _is_whitespace\n",
      "    cat = unicodedata.category(char)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in <genexpr>\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"<ipython-input-53-208ac9ba460e>\", line 13, in process_chunk\n",
      "    doc_content = tokenizer.tokenize(getcontent(doc_id, tokenized_docs))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 258, in tokenize\n",
      "    text = self._clean_text(text)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 351, in _clean_text\n",
      "    if cp == 0 or cp == 0xfffd or _is_control(char):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 439, in _is_control\n",
      "    if cat.startswith(\"C\"):\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 324, in _is_chinese_char\n",
      "    def _is_chinese_char(self, cp):\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 446, in _is_punctuation\n",
      "    cp = ord(char)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 170, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 640, in tokenize\n",
      "    tokenized_text = split_on_tokens(added_tokens, text)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in split_on_tokens\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\", line 637, in <genexpr>\n",
      "    else [token] for token in tokenized_text), [])\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 171, in _tokenize\n",
      "    for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 396, in tokenize\n",
      "    while start < len(chars):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 170, in _tokenize\n",
      "    for token in self.basic_tokenizer.tokenize(text, never_split=self.all_special_tokens):\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 351, in _clean_text\n",
      "    if cp == 0 or cp == 0xfffd or _is_control(char):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 439, in _is_control\n",
      "    if cat.startswith(\"C\"):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 273, in tokenize\n",
      "    split_tokens.extend(self._run_split_on_punc(token))\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 299, in _run_split_on_punc\n",
      "    if _is_punctuation(char):\n",
      "KeyboardInterrupt\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 351, in _clean_text\n",
      "    if cp == 0 or cp == 0xfffd or _is_control(char):\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 272, in tokenize\n",
      "    token = self._run_strip_accents(token)\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 436, in _is_control\n",
      "    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
      "  File \"/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\", line 281, in _run_strip_accents\n",
      "    output = []\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "pool = mp.Pool(n_cpus)\n",
    "jobs = []\n",
    "\n",
    "def process_chunk(chunk_no, n_itens, max_lines):\n",
    "    starting_index = chunk_no*n_itens\n",
    "    end_index = min(((chunk_no+1)*n_itens)-1, max_lines)\n",
    "    outfile = \"/ssd2/arthur/TREC2019/data/triples-tokenized/cut-train.top100.{}\".format(chunk_no)\n",
    "    with open(outfile, 'w') as outf:\n",
    "        for topic_id, doc_id in tqdm(all_docs[starting_index:end_index], position = chunk_no+1):\n",
    "            _id = \"{}-{}\".format(topic_id, doc_id)\n",
    "            doc_content = tokenizer.tokenize(getcontent(doc_id, tokenized_docs))\n",
    "            label = \"1\" if _id in qrels else \"0\"\n",
    "            query = queries[topic_id]\n",
    "            truncate_seq_pair(query, doc_content)\n",
    "            tokens = [\"[CLS]\"] + query + [\"[SEP]\"] + doc_content + [\"[SEP]\"]\n",
    "            outf.write(\"{}\\t{}\\t{}\\n\".format(_id, tokens, label))\n",
    "\n",
    "def update(*a):\n",
    "    pbar.update()\n",
    "pbar = tqdm(total=number_of_chunks)\n",
    "for i in range(number_of_chunks):\n",
    "    jobs.append(pool.apply_async(process_chunk, args=(i, chunk_size, len(all_docs)), callback=update))\n",
    "pbar.close()\n",
    "for job in jobs:\n",
    "        job.get()\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T13:57:19.666643Z",
     "start_time": "2019-09-23T13:57:19.663437Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['watts', '&', 'browning', 'engineers']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T13:56:36.076520Z",
     "start_time": "2019-09-23T13:56:19.932504Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e08a0797ce344f07940531e773a68d3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=36701300), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-da2c3da6f46e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mtopic_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mtuple_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{}-{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mdoc_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetcontent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"1\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtuple_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mqrels\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtopic_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0madded_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madded_tokens_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m         \u001b[0mtokenized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_on_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madded_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\u001b[0m in \u001b[0;36msplit_on_tokens\u001b[0;34m(tok_list, text)\u001b[0m\n\u001b[1;32m    635\u001b[0m             return sum((self._tokenize(token, **kwargs) if token not \\\n\u001b[1;32m    636\u001b[0m                     \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madded_tokens_encoder\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_tokens\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m                     else [token] for token in tokenized_text), [])\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0madded_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madded_tokens_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    635\u001b[0m             return sum((self._tokenize(token, **kwargs) if token not \\\n\u001b[1;32m    636\u001b[0m                     \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madded_tokens_encoder\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_tokens\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 637\u001b[0;31m                     else [token] for token in tokenized_text), [])\n\u001b[0m\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0madded_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madded_tokens_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\u001b[0m in \u001b[0;36m_tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0msplit_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_basic_tokenize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnever_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msub_token\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwordpiece_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                     \u001b[0msplit_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, never_split)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \"\"\"\n\u001b[1;32m    257\u001b[0m         \u001b[0mnever_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnever_split\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnever_split\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnever_split\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m         \u001b[0;31m# This was added on November 1st, 2018 for the multilingual and Chinese\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# models. This is also applied to the English models now, but it doesn't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\u001b[0m in \u001b[0;36m_clean_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0mcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mcp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0xfffd\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_is_control\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_is_whitespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/bert/lib/python3.7/site-packages/pytorch_transformers/tokenization_bert.py\u001b[0m in \u001b[0;36m_is_control\u001b[0;34m(char)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\\t\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\\r\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m     \u001b[0mcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0municodedata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Generate top100 triples from runs\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "split = \"train\"\n",
    "\n",
    "# tokenized_queries = {}\n",
    "# run_file = \"/ssd2/arthur/TREC2019/data/runs/indri_{}.run\".format(split)\n",
    "# qrels_file = \"/ssd2/arthur/TREC2019/data/qrels/{}_qrels\".format(split)\n",
    "# qrels = set()\n",
    "# for line in tqdm(open(qrels_file)):\n",
    "#     query_id, _, doc_id, label = line.strip().split()\n",
    "#     tuple_id = \"{}-{}\".format(query_id, doc_id)\n",
    "#     if label==\"1\":\n",
    "#         qrels.add(tuple_id)\n",
    "\n",
    "# queries = {}\n",
    "# for line in tqdm(open(\"/ssd2/arthur/TREC2019/data/queries/{}_queries.tsv\".format(split))):\n",
    "#     query_id, query_text = line.strip().split(\"\\t\")\n",
    "#     queries[query_id] = tokenizer.tokenize(query_text)\n",
    "\n",
    "tokenized_docs = \"/ssd2/arthur/TREC2019/data/docs/tokenized-msmarco-docs.tsv\"\n",
    "processed_per_topic = Counter()\n",
    "with open(\"/ssd2/arthur/TREC2019/data/triples-tokenized/cut-{}.top100\".format(split), 'w') as outf:\n",
    "    for line in tqdm(open(run_file), total = 100*len(queries)):\n",
    "        topic_id, _, doc_id, _, _, _ = line.split()\n",
    "        tuple_id = \"{}-{}\".format(topic_id, doc_id)\n",
    "        doc_content = tokenizer.tokenize(getcontent(doc_id, tokenized_docs))\n",
    "        label = \"1\" if tuple_id in qrels else \"0\"\n",
    "        query = queries[topic_id]\n",
    "        truncate_seq_pair(query, doc_content)\n",
    "        tokens = [\"[CLS]\"] + query + [\"[SEP]\"] + doc_content + [\"[SEP]\"]\n",
    "        outf.write(\"{}\\t{}\\t{}\\n\".format(tuple_id, tokens, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T19:36:22.730547Z",
     "start_time": "2019-09-23T19:36:17.713465Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027019d199f6462c935600004bff248a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=36701300), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#top 10\\\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from tqdm.auto import tqdm\n",
    "from pytorch_transformers import BertTokenizer\n",
    "import pickle\n",
    "offset_dict = pickle.load(open(\"/ssd2/arthur/TREC2019/data/docs/tokenized-msmarco-docs.tsv.offset\", 'rb'))\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "split = \"train\"\n",
    "qrels_file = \"/ssd2/arthur/TREC2019/data/qrels/{}_qrels\".format(split)\n",
    "\n",
    "qrels = defaultdict(lambda:set())\n",
    "for line in open(qrels_file):\n",
    "    query_id, _, doc_id, label = line.strip().split()\n",
    "    tuple_id = \"{}-{}\".format(query_id, doc_id)\n",
    "    if label==\"1\":\n",
    "        qrels[query_id].add(doc_id)\n",
    "\n",
    "last_topic = -1\n",
    "top_100 = []\n",
    "n_negatives = 10\n",
    "with open(\"/ssd2/arthur/TREC2019/data/triples-tokenized/cut-{}.{}neg\".format(split, n_negatives), 'w') as outf:\n",
    "    for line in tqdm(open(\"/ssd2/arthur/TREC2019/data/triples-tokenized/cut-{}.top100\".format(split)), total = 100*len(qrels)):\n",
    "        guid, doc, label = line.strip().split(\"\\t\")\n",
    "        topic_id, doc_id = guid.split(\"-\")\n",
    "        if topic_id != last_topic and last_topic != -1:\n",
    "            for positive in qrels[last_topic]:\n",
    "                tokenized_positive = tokenizer.tokenize(getcontent(positive, tokenized_docs))\n",
    "                outf.write(\"{}-{}\\t{}\\t1\\n\".format(last_topic, positive, tokenized_positive))\n",
    "                random_neg = random.sample(top_100, n_negatives)\n",
    "                for neg in random_neg:\n",
    "                    outf.write(\"{}\\t{}\\t0\\n\".format(neg[0], neg[1]))\n",
    "            top_100 = []\n",
    "        if label != \"1\":\n",
    "            top_100.append((guid, eval(doc), label))\n",
    "        last_topic = topic_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T13:56:16.377611Z",
     "start_time": "2019-09-23T13:56:16.372479Z"
    }
   },
   "outputs": [],
   "source": [
    "def truncate_seq_pair(tokens_a, tokens_b, max_length=509):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T18:28:21.838708Z",
     "start_time": "2019-09-24T18:27:19.538583Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b0f209776ef42d798fec3c1ae79144c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from pytorch_transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "split = \"train\"\n",
    "queries = {}\n",
    "for line in tqdm(open(\"/ssd2/arthur/TREC2019/data/queries/{}_queries.tsv\".format(split))):\n",
    "    query_id, query_text = line.strip().split(\"\\t\")\n",
    "    queries[query_id] = tokenizer.tokenize(query_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T18:29:55.554106Z",
     "start_time": "2019-09-24T18:29:55.548002Z"
    }
   },
   "outputs": [],
   "source": [
    "def truncate_seq_pair(tokens_a, tokens_b, max_length=509):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T18:33:57.101607Z",
     "start_time": "2019-09-24T18:29:56.314072Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"/ssd2/arthur/TREC2019/data/triples-tokenized/cut-{}.10neg.fix\".format(split), 'w') as outf:\n",
    "    for line in open(\"/ssd2/arthur/TREC2019/data/triples-tokenized/cut-{}.10neg\".format(split)):\n",
    "        if \"[SEP]\" in line:\n",
    "            outf.write(line)\n",
    "        else:\n",
    "            _id, doc, label = line.strip().split(\"\\t\")\n",
    "            topic_id, doc_id = _id.split(\"-\")\n",
    "            query_text = queries[topic_id]\n",
    "            doc = eval(doc)\n",
    "            truncate_seq_pair(query_text, doc)\n",
    "            final_doc = [\"[CLS]\"] + query_text + [\"[SEP]\"] + doc + [\"[SEP]\"]\n",
    "            outf.write(f\"{_id}\\t{final_doc}\\t{label}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
