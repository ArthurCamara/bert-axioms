{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T01:31:24.736073Z",
     "start_time": "2019-10-11T01:31:23.720857Z"
    }
   },
   "outputs": [],
   "source": [
    "import pytorch_transformers\n",
    "import sys\n",
    "import torch\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "sys.path.insert(0, \"/ssd2/arthur/TREC2019/scripts/\")\n",
    "from msmarco_dataset import MsMarcoDataset\n",
    "from args_parser import getArgs\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from tqdm.autonotebook import tqdm                         \n",
    "from sklearn.metrics import f1_score, average_precision_score, accuracy_score\n",
    "import numpy as np\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2,3,4,5,6,7\"  # specify which GPU(s) to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T01:31:27.971879Z",
     "start_time": "2019-10-11T01:31:25.407066Z"
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_transformers import DistilBertForSequenceClassification\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"/ssd2/arthur/TREC2019/data/models/distilbert-model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T01:31:29.439336Z",
     "start_time": "2019-10-11T01:31:27.974781Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf6aba73bbf441c388c7541fe2c61be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Counting lines on file...', max=1, style=Pr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"/ssd2/arthur/TREC2019/data\"\n",
    "\n",
    "labeled=True\n",
    "datasets_names = [\"test\"]\n",
    "\n",
    "batch_size = 1024\n",
    "dataloaders = {}\n",
    "datasets = {}\n",
    "\n",
    "for dataset in datasets_names:\n",
    "    _file = os.path.join(data_dir, \"triples-tokenized\", \"cut-test.top100\")\n",
    "    assert os.path.isfile(_file)\n",
    "    datasets[dataset] = MsMarcoDataset(_file, data_dir, distil=True, invert_label=True, labeled=labeled)\n",
    "    dataloaders[dataset] = DataLoader(datasets[dataset], batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T01:31:42.137829Z",
     "start_time": "2019-10-11T01:31:36.773142Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "n_gpu = torch.cuda.device_count()\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available() and n_gpu > 0) else \"cpu\")\n",
    "model = torch.nn.DataParallel(model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T01:39:04.907437Z",
     "start_time": "2019-10-11T01:31:43.449910Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eacd3c8f28c4038a39d1405c313330a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='test Dataset', max=153, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arthur/miniconda3/envs/bert/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\taccuracy: 0.9814453125\n",
      "\tf1_score: 0.9906265416872224\n",
      "\taccuracy: 0.9833984375\n",
      "\tf1_score: 0.9915966386554622\n",
      "\taccuracy: 0.982421875\n",
      "\tf1_score: 0.9910979228486646\n",
      "\taccuracy: 0.9775390625\n",
      "\tf1_score: 0.9886082218920258\n",
      "\taccuracy: 0.98046875\n",
      "\tf1_score: 0.9901185770750986\n",
      "\taccuracy: 0.9736328125\n",
      "\tf1_score: 0.9866137828458107\n",
      "\taccuracy: 0.9765625\n",
      "\tf1_score: 0.9881305637982196\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "preds = {}\n",
    "for dataset in datasets_names:\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds[dataset] = None\n",
    "    out_label_ids = 0\n",
    "    _preds = None\n",
    "    dataloader = dataloaders[dataset]\n",
    "    for index, batch in tqdm(enumerate(dataloader), desc=\"{} Dataset\".format(dataset), total = len(dataloader)):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            if labeled:\n",
    "                inputs = {'input_ids': batch[0].to(device),\n",
    "                          'attention_mask': batch[1].to(device),\n",
    "                          'labels': batch[3].to(device)}\n",
    "            else:\n",
    "                inputs = {'input_ids': batch[0].to(device),\n",
    "                          'attention_mask': batch[1].to(device)}\n",
    "            outputs = model(**inputs)\n",
    "            if labeled:\n",
    "                tmp_eval_loss, logits = outputs[:2]\n",
    "                eval_loss+=tmp_eval_loss.mean().item()\n",
    "                nb_eval_steps+=1\n",
    "            else:\n",
    "                logits = outputs[0]\n",
    "\n",
    "            if _preds is None:\n",
    "                _preds = logits.detach().cpu().numpy()\n",
    "                if labeled:\n",
    "                    out_label_ids = inputs['labels'].detach().cpu().numpy().flatten()\n",
    "\n",
    "            else:\n",
    "                batch_predictions = logits.detach().cpu().numpy()\n",
    "                _preds = np.append(_preds, batch_predictions, axis=0)\n",
    "                if labeled:\n",
    "                    batch_ground_truth = inputs['labels'].detach().cpu().numpy().flatten()\n",
    "                    out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy().flatten(), axis=0)\n",
    "                    if index%20 == 0:\n",
    "                        print(\"\\taccuracy: {}\".format(accuracy_score(batch_ground_truth, np.argmax(batch_predictions, axis=1))))\n",
    "                        print(\"\\tf1_score: {}\".format(f1_score(batch_ground_truth, np.argmax(batch_predictions, axis=1))))\n",
    "    if labeled:\n",
    "        assert len(_preds) == len(out_label_ids)\n",
    "    torch.save(_preds, os.path.join(data_dir, 'predictions', '{}_.distilBERT.tensor'.format(dataset)))\n",
    "    preds[dataset] = softmax(torch.as_tensor(_preds))[:,0].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T01:42:10.287692Z",
     "start_time": "2019-10-11T01:42:10.245609Z"
    }
   },
   "outputs": [],
   "source": [
    "preds[dataset] = softmax(torch.as_tensor(_preds)).cpu().numpy()[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T01:42:15.552871Z",
     "start_time": "2019-10-11T01:42:12.216088Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ddb2b3f83354cdda6fe03470dd8f1ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='reading run file', max=155800, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed1f121829aa48e2b6985147ed68e462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='normalizing', max=1558, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "155800\n"
     ]
    }
   ],
   "source": [
    "#load QL scores and normalize\n",
    "from collections import defaultdict\n",
    "ql_scores = defaultdict(lambda:defaultdict(lambda:[]))\n",
    "ordered_topics = defaultdict(lambda:[])\n",
    "scores_per_topic = defaultdict(lambda:defaultdict(lambda:[]))\n",
    "for dataset in datasets_names:\n",
    "    QL_run_file = \"/ssd2/arthur/TREC2019/data/runs/indri_test_10_10.run\"\n",
    "    last_topic = None\n",
    "    normalized_scores = []\n",
    "\n",
    "    with open(QL_run_file, 'r') as inf:\n",
    "        for counter, line in tqdm(enumerate(inf), desc=\"reading run file\", total=len(datasets[dataset])):\n",
    "            [topic_id, _, doc_id, _, score, _] = line.split()\n",
    "            if topic_id not in ordered_topics[dataset]:\n",
    "                ordered_topics[dataset].append(topic_id)\n",
    "            scores_per_topic[dataset][topic_id].append((doc_id, score))\n",
    "    #normalize\n",
    "    for _id in tqdm(scores_per_topic[dataset], desc=\"normalizing\"):\n",
    "        _scores = np.asarray([float(x[1]) for x in scores_per_topic[dataset][_id]])\n",
    "        normalized_scores = (_scores - np.min(_scores))/np.ptp(_scores)\n",
    "        for (did, _), score in zip(scores_per_topic[dataset][_id], normalized_scores):\n",
    "            guid = \"{}-{}\".format(_id, did)\n",
    "            ql_scores[dataset][guid] = score\n",
    "print(len(ql_scores[\"test\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-11T01:42:25.284628Z",
     "start_time": "2019-10-11T01:42:19.667712Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\talpha: 0.0\t ndcg: 0.2179\n",
      "\talpha: 0.85\t ndcg: 0.2881\n",
      "\talpha: 1.0\t ndcg: 0.2816\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "trec_eval_path = \"/ssd2/arthur/trec_eval/trec_eval\"\n",
    "dev_qrel_path = \"/ssd2/arthur/TREC2019/data/qrels/test_qrels\"\n",
    "cmd = \"{} -q -c {} {}\"\n",
    "map_cmd = \"{} -q -m map {} {}\"\n",
    "ndcg_cmd = \"{} -q -m ndcg {} {}\"\n",
    "best_map = 0.0\n",
    "best_ndcg = 0.0\n",
    "runs_format = \"{} Q0 {} {} {} DISTILBERT_QL\\n\" #topic_id, doc_id, ranking, score\n",
    "# preds[dataset] = softmax(torch.as_tensor(_preds))[:,1].cpu().numpy()\n",
    "# alphas = [0.0, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, .45, .5, .55, .6, .65, .7, .75, .8, 0.85, .9, .95, 1.0]\n",
    "alphas = [0.0, 0.85, 1.0]\n",
    "for dataset in datasets:\n",
    "#     ql_run_file = os.path.join(\"/ssd2/arthur/TREC2019/data/runs/{}_QL.run\".format(dataset))\n",
    "    ql_run_file = \"/ssd2/arthur/TREC2019/data/runs/indri_test_10_10.run\"\n",
    "    for alpha in alphas:\n",
    "        beta = 1-alpha\n",
    "        out_run_file = os.path.join(\"/ssd2/arthur/TREC2019/data/runs/{}_distilBert-{}.run\".format(dataset, alpha))\n",
    "        topic_results = [] \n",
    "        last_topic = -1\n",
    "        with open(ql_run_file, 'r') as inf, open(out_run_file, 'w') as outf:\n",
    "            for counter, (example, score) in enumerate(zip(inf, preds[dataset])):\n",
    "                topic_id, _, doc_id, _, _, _ = example.split()\n",
    "                guid = \"{}-{}\".format(topic_id, doc_id)\n",
    "                if topic_id != last_topic and len(topic_results) > 0:\n",
    "                    topic_results.sort(key=lambda x:x['score'], reverse=True)\n",
    "                    for rank, topic in enumerate(topic_results):\n",
    "                        outf.write(runs_format.format(topic['topic_id'], topic['doc_id'], rank, topic['score']))\n",
    "                    topic_results = []\n",
    "                topic_results.append({'topic_id':topic_id, 'doc_id':doc_id, 'score':alpha*score + beta*ql_scores[dataset][guid]})\n",
    "                last_topic = topic_id\n",
    "\n",
    "            #dump last topic\n",
    "            topic_results.sort(key=lambda x:x['score'], reverse=True)\n",
    "            for rank, topic in enumerate(topic_results):\n",
    "                outf.write(runs_format.format(topic['topic_id'], topic['doc_id'], rank, topic['score']))\n",
    "        \n",
    "        if labeled:\n",
    "            qrel_file = os.path.join(data_dir, 'qrels', '{}_qrels'.format(dataset))\n",
    "            ndcgs = subprocess.check_output(ndcg_cmd.format(trec_eval_path, qrel_file, out_run_file).split()).decode('utf-8')\n",
    "#             maps = [float(x.strip().split(\"\\t\")[-1]) for x in maps.split(\"\\n\") if len(x)>2]\n",
    "            _ndcg = float(ndcgs.split(\"\\n\")[-2].split(\"\\t\")[-1])\n",
    "            print(\"\\talpha: {}\\t ndcg: {}\".format(alpha, _ndcg))\n",
    "            if _ndcg > best_ndcg:\n",
    "                best_ndcg = _ndcg\n",
    "                best_alpha = alpha\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndcgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T23:25:55.030950Z",
     "start_time": "2019-10-09T23:25:55.001305Z"
    }
   },
   "outputs": [],
   "source": [
    "ql_scores[dataset][guid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T08:36:46.690323Z",
     "start_time": "2019-10-09T08:36:46.674398Z"
    }
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "trec_eval_path = \"/ssd2/arthur/trec_eval/trec_eval\"\n",
    "dev_qrel_path = \"/ssd2/arthur/TREC2019/data/qrels/test_qrels\"\n",
    "cmd = \"{} -q -c {} {}\"\n",
    "map_cmd = \"{} -q -m ndcg {} {}\"\n",
    "best_map = 0.0\n",
    "runs_format = \"{} Q0 {} {} {} DISTILBERT_QL\\n\" #topic_id, doc_id, ranking, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T08:41:21.168089Z",
     "start_time": "2019-10-09T08:41:20.482785Z"
    }
   },
   "outputs": [],
   "source": [
    "qrel_file = os.path.join(data_dir, 'qrels', 'test_qrels')\n",
    "for alpha in [0.0, 0.85, 1.0]:\n",
    "    out_run_file = os.path.join(\"/ssd2/arthur/TREC2019/data/runs/{}_distilBert-{}.run\".format(dataset, alpha))\n",
    "    print(out_run_file)\n",
    "    map_cmd = \"{} -q -m recip_rank {} {}\"\n",
    "    ndcgs = subprocess.check_output(map_cmd.format(trec_eval_path, qrel_file, out_run_file).split()).decode('utf-8')\n",
    "    _ndcg = float(ndcgs.split(\"\\n\")[-2].split(\"\\t\")[-1])\n",
    "    print(\"\\talpha: {}\\t ndcg: {}\".format(alpha, _ndcg))\n",
    "# maps = subprocess.check_output(map_cmd.format(trec_eval_path, qrel_file, out_run_file).split()).decode('utf-8')\n",
    "# maps = [float(x.strip().split(\"\\t\")[-1]) for x in maps.split(\"\\n\") if len(x)>2]\n",
    "# _map = float(result.split(\"\\n\")[-26].split(\"\\t\")[-1])\n",
    "# print(\"\\talpha: {}\\t map: {}\".format(alpha, _map))\n",
    "# if _map > best_map:\n",
    "#     best_map = _map\n",
    "#     best_alpha = alpha\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-09T08:38:25.963545Z",
     "start_time": "2019-10-09T08:38:25.932702Z"
    }
   },
   "outputs": [],
   "source": [
    "float(maps.split(\"\\n\")[-2].split(\"\\t\")[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
