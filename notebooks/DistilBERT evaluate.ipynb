{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T13:05:07.719400Z",
     "start_time": "2019-09-11T13:05:07.673711Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pytorch_transformers\n",
    "import sys\n",
    "import torch\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "sys.path.insert(0, \"/ssd2/arthur/TREC2019/scripts/\")\n",
    "from msmarco_dataset import MsMarcoDataset\n",
    "from args_parser import getArgs\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from tqdm.autonotebook import tqdm                         \n",
    "from sklearn.metrics import f1_score, average_precision_score, accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T11:53:52.232433Z",
     "start_time": "2019-09-11T11:53:48.150876Z"
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_transformers import DistilBertForSequenceClassification\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"/ssd2/arthur/TREC2019/data/models/distilbert-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T13:04:04.638631Z",
     "start_time": "2019-09-11T13:04:04.603227Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = \"/ssd2/arthur/TREC2019/data\"\n",
    "\n",
    "batch_size = 2048\n",
    "\n",
    "dev_file = os.path.join(data_dir, \"triples-tokenized\", \"dev-triples.top100\")\n",
    "assert os.path.isfile(dev_file)\n",
    "\n",
    "test_file = os.path.join(data_dir, \"triples-tokenized\", \"test-triples.top100\")\n",
    "assert os.path.isfile(test_file)\n",
    "\n",
    "fulldev_file = os.path.join(data_dir, \"triples-tokenized\", \"fulldev-triples.top100\")\n",
    "assert os.path.isfile(fulldev_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T13:04:37.710619Z",
     "start_time": "2019-09-11T13:04:07.938955Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15cea026bf7f4a2292e460fb75f5e1bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Counting lines on file...', max=1, style=Pr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b260836956e4559927719b0088a95a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Computing offset dictionary', max=363500, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b7580005dad4e78843f2d188c9c0390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Counting lines on file...', max=1, style=Pr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e16a904b454d51911881f1029b1071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Computing offset dictionary', max=155800, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef6c015e01c43ebade50ab87c9e84b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', description='Counting lines on file...', max=1, style=Pr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5691940bb8d443e97b4d1dfeb44d30f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Computing offset dictionary', max=519300, style=ProgressStyle…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dev_dataset = MsMarcoDataset(dev_file, data_dir, distil=True, invert_label=True, force=True)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
    "assert len(dev_dataset)==363500\n",
    "\n",
    "\n",
    "test_dataset = MsMarcoDataset(test_file, data_dir, distil=True, invert_label=True, force=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "assert len(test_dataset)==155800\n",
    "\n",
    "fulldev_dataset = MsMarcoDataset(fulldev_file, data_dir, distil=True, invert_label=True, force=True)\n",
    "fulldev_dataloader = DataLoader(fulldev_dataset, batch_size=batch_size, shuffle=False)\n",
    "assert len(fulldev_dataset) == len(dev_dataset) + len(test_dataset)\n",
    "\n",
    "dataloaders = {'dev': dev_dataloader, 'test': test_dataloader, 'fulldev': fulldev_dataloader}\n",
    "datasets = {'dev': dev_dataset, 'test': test_dataset, 'fulldev': fulldev_dataset}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T11:56:38.231545Z",
     "start_time": "2019-09-11T11:56:36.085505Z"
    }
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "n_gpu = torch.cuda.device_count()\n",
    "device = torch.device(\"cuda\" if (torch.cuda.is_available() and n_gpu > 0) else \"cpu\")\n",
    "model = torch.nn.DataParallel(model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T13:08:10.171211Z",
     "start_time": "2019-09-11T13:08:09.927860Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "preds = {}\n",
    "for dataset in ['test', 'dev', 'fulldev']:\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds[dataset] = None\n",
    "    out_label_ids = 0\n",
    "    _preds = None\n",
    "    if os.path.isfile(os.path.join(data_dir, 'predictions', '{}_distilBERT.tensor'.format(dataset))):\n",
    "        preds[dataset] = torch.load(os.path.join(data_dir, 'predictions', '{}_distilBERT.tensor'.format(dataset)))\n",
    "        preds[dataset] = list(softmax(torch.as_tensor(preds[dataset]))[:, 0].cpu().numpy())\n",
    "        continue\n",
    "    dataloader = dataloaders[dataset]\n",
    "    for index, batch in tqdm(enumerate(dataloader), desc=\"{} Dataset\".format(dataset), total = len(dataloader)):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs = {'input_ids': batch[0].to(device),\n",
    "                      'attention_mask': batch[1].to(device),\n",
    "                      'labels': batch[3].to(device)}\n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "            eval_loss+=tmp_eval_loss.mean().item()\n",
    "            nb_eval_steps+=1\n",
    "\n",
    "            if _preds is None:\n",
    "                _preds = logits.detach().cpu().numpy()\n",
    "                out_label_ids = inputs['labels'].detach().cpu().numpy().flatten()\n",
    "\n",
    "            else:\n",
    "                batch_predictions = logits.detach().cpu().numpy()\n",
    "                batch_ground_truth = inputs['labels'].detach().cpu().numpy().flatten()\n",
    "                if index%50 == 0:\n",
    "                    print(\"\\taccuracy: {}\".format(accuracy_score(batch_ground_truth, np.argmax(batch_predictions, axis=1))))\n",
    "                    print(\"\\tf1_score: {}\".format(f1_score(batch_ground_truth, np.argmax(batch_predictions, axis=1))))\n",
    "                _preds = np.append(_preds, batch_predictions, axis=0)\n",
    "                out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy().flatten(), axis=0)\n",
    "\n",
    "    assert len(_preds) == len(out_label_ids)\n",
    "    torch.save(_preds, os.path.join(data_dir, 'predictions', '{}_distilBERT.tensor'.format(dataset)))\n",
    "    preds[dataset] = _preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T13:06:02.568185Z",
     "start_time": "2019-09-11T13:05:12.376125Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e22532d22c4c19a72bf35332b27037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='reading run file', max=155800, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5868efb3cadd4281a6aaa61f80ecb0e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='normalizing', max=1558, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96135eb8fbd0466aba95c6e4e9aeb43e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='reading run file', max=363500, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83af66719b174bc8b6d604a5524b95de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='normalizing', max=3635, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f3b4fb943cb4e43880525de027f8af4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='reading run file', max=519300, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "767c57091d28468ea23a026c3782e42d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='normalizing', max=5193, style=ProgressStyle(description_width…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load QL scores and normalize\n",
    "\n",
    "from collections import defaultdict\n",
    "ql_scores = defaultdict(lambda:defaultdict(lambda:[]))\n",
    "ordered_topics = defaultdict(lambda:[])\n",
    "scores_per_topic = defaultdict(lambda:defaultdict(lambda:[]))\n",
    "for dataset in ['test', 'dev', 'fulldev']:\n",
    "    QL_run_file = \"/ssd2/arthur/TREC2019/data/runs/{}_QL.run\".format(dataset)\n",
    "    last_topic = None\n",
    "    normalized_scores = []\n",
    "\n",
    "    with open(QL_run_file, 'r') as inf:\n",
    "        for counter, line in tqdm(enumerate(inf), desc=\"reading run file\", total=len(datasets[dataset])):\n",
    "            [topic_id, _, doc_id, _, score, _] = line.split()\n",
    "            if topic_id not in ordered_topics[dataset]:\n",
    "                ordered_topics[dataset].append(topic_id)\n",
    "            scores_per_topic[dataset][topic_id].append((doc_id, score))\n",
    "    #normalize\n",
    "    for _id in tqdm(scores_per_topic[dataset], desc=\"normalizing\"):\n",
    "        _scores = np.asarray([float(x[1]) for x in scores_per_topic[dataset][_id]])\n",
    "        normalized_scores = (_scores - np.min(_scores))/np.ptp(_scores)\n",
    "        for (did, _), score in zip(scores_per_topic[dataset][_id], normalized_scores):\n",
    "            guid = \"{}-{}\".format(_id, did)\n",
    "            ql_scores[dataset][guid] = score\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T13:20:00.397823Z",
     "start_time": "2019-09-11T13:19:30.302532Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev\n",
      "\talpha: 0.0\t map: 0.2196\n",
      "\talpha: 0.85\t map: 0.375\n",
      "\talpha: 1.0\t map: 0.3638\n",
      "test\n",
      "\talpha: 0.0\t map: 0.2274\n",
      "\talpha: 0.85\t map: 0.3833\n",
      "\talpha: 1.0\t map: 0.3729\n",
      "fulldev\n",
      "\talpha: 0.0\t map: 0.2219\n",
      "\talpha: 0.85\t map: 0.3775\n",
      "\talpha: 1.0\t map: 0.3665\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "trec_eval_path = \"/ssd2/arthur/trec_eval/trec_eval\"\n",
    "dev_qrel_path = \"/ssd2/arthur/TREC2019/data/qrels/dev_qrels\"\n",
    "cmd = \"{} -q -c {} {}\"\n",
    "map_cmd = \"{} -q -m map {} {}\"\n",
    "best_map = 0.0\n",
    "runs_format = \"{} Q0 {} {} {} DISTILBERT_QL\\n\" #topic_id, doc_id, ranking, score\n",
    "\n",
    "alphas = [0.0, 0.85, 1.0]\n",
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    ql_run_file = os.path.join(\"/ssd2/arthur/TREC2019/data/runs/{}_QL.run\".format(dataset))\n",
    "    for alpha in alphas:\n",
    "        beta = 1-alpha\n",
    "        out_run_file = os.path.join(\"/ssd2/arthur/TREC2019/data/runs/{}_distilBert-{}.run\".format(dataset, alpha))\n",
    "        topic_results = [] \n",
    "        last_topic = -1\n",
    "        with open(ql_run_file, 'r') as inf, open(out_run_file, 'w') as outf:\n",
    "            for counter, (example, score) in enumerate(zip(inf, preds[dataset])):\n",
    "                topic_id, _, doc_id, _, _, _ = example.split()\n",
    "                guid = \"{}-{}\".format(topic_id, doc_id)\n",
    "                if topic_id != last_topic and len(topic_results) > 0:\n",
    "                    topic_results.sort(key=lambda x:x['score'], reverse=True)\n",
    "                    for rank, topic in enumerate(topic_results):\n",
    "                        outf.write(runs_format.format(topic['topic_id'], topic['doc_id'], rank, topic['score']))\n",
    "                    topic_results = []\n",
    "                topic_results.append({'topic_id':topic_id, 'doc_id':doc_id, 'score':alpha*score + beta*ql_scores[dataset][guid]})\n",
    "                last_topic = topic_id\n",
    "\n",
    "            #dump last topic\n",
    "            topic_results.sort(key=lambda x:x['score'], reverse=True)\n",
    "            for rank, topic in enumerate(topic_results):\n",
    "                outf.write(runs_format.format(topic['topic_id'], topic['doc_id'], rank, topic['score']))\n",
    "        \n",
    "        qrel_file = os.path.join(data_dir, 'qrels', '{}_qrels'.format(dataset))\n",
    "        result = subprocess.check_output(cmd.format(trec_eval_path, qrel_file , out_run_file).split()).decode('utf-8')\n",
    "        maps = subprocess.check_output(map_cmd.format(trec_eval_path, qrel_file, out_run_file).split()).decode('utf-8')\n",
    "        maps = [float(x.strip().split(\"\\t\")[-1]) for x in maps.split(\"\\n\") if len(x)>2]\n",
    "        _map = float(result.split(\"\\n\")[-26].split(\"\\t\")[-1])\n",
    "        print(\"\\talpha: {}\\t map: {}\".format(alpha, _map))\n",
    "        if _map > best_map:\n",
    "            best_map = _map\n",
    "            best_alpha = alpha\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
